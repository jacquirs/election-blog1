<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Post Election Reflection | A minimal Hugo website</title>
    <link rel="stylesheet" href="/css/style.css" />
    <link rel="stylesheet" href="/css/fonts.css" />
    
  </head>

  <body>
    <nav>
    <ul class="menu">
      
      <li><a href="/">Home</a></li>
      
      <li><a href="/about/">About</a></li>
      
      <li><a href="/categories/">Categories</a></li>
      
      <li><a href="/tags/">Tags</a></li>
      
      <li><a href="/index.xml">Subscribe</a></li>
      
    </ul>
    <hr/>
    </nav>

<div class="article-meta">
<h1><span class="title">Post Election Reflection</span></h1>
<h2 class="author">Jacqui Schlesinger</h2>
<h2 class="date">2024/11/10</h2>
</div>

<main>



<p><em>With the election finally concluded and all states accounted for, I now take on the task of evaluating my final and other weeks’ models to determine their accuracy and understand why the election outcome was so far from what I predicted.</em></p>
<p>While my final prediction had Harris winning by a healthy margin in the electoral college, as I return to my models post election I see that the result is not completely different from what I predicted. Almost every model I have every created had either candidate winning within the margin of error for almost every state. Though my final model was less conservative than my past weeks, as noted this was partially due to last minute changes I had to make in my ensembling and modeling approach. Overall, though my predictions overtime all chose Kamala Harris to win the presidency, a Trump win was in the margin of error, but as we will evaluate below a win by this much was unexpected given my model. I will evaluate how this could have happened and what steps I could take to improve on this below.</p>
<div id="review-of-models-and-predictions" class="section level3">
<h3>Review of Models and Predictions</h3>
<p>In this section, I review the forecasting models developed throughout weeks 1 to 9 and the results each yielded. Each model was built with a unique combination of variables and assumptions, leading to varying results. By walking through the results and calculations from each week, I reflect on which model performed best and why. While most of these were not in my final prediction, we will look through each step in the week-by-week decision making process where assumptions or decisions may have led to overfitting, bias, and other contributors to my prediction’s differences. Some of these models were reviewed and updated with new data through week 6, making their predictions more timely.</p>
<p><strong>Week 1:</strong> This week used a simplified Norpoth model to make a prediction based on a weighted average of the election results from 2016 and 2020 by state. The predictive model can be defined as <span class="math inline">\(vote_{2024} = 0.75vote_{2020} + 0.25vote_{2016}\)</span>. This forecast resulted in Harris 276 - Trump 262.</p>
<p><strong>Week 2:</strong> This week focused on economic variables and linear regressions. Evaluating a variety of economic fundamentals as predictors of two way popular vote, I found Q2 GDP growth in the election year to be the best predictor by on R sqaured, RMSE, and a variety of other linear model evaluations as compared to other economic variables. This forecast focused on two way popular vote, leading to a result of <em>Harris 51.585% - Trump 48.415%</em>.</p>
<p><strong>Week 3:</strong> The main two models developed this week were ensembled elastic net regression models that weighed fundamentals more closer to the election and weighed polling more closer to the election. The final forecast for this week was an unweighted average between the two, leading to <em>Harris 52.25% - Trump 49.85%</em> when updated with new polling data.</p>
<p><strong>Week 4:</strong> During this week, I began to narrow my work only down to the seven critical states: Arizona, Georgia, Michigan, Nevada, North Carolina, Pennsylvania, and Wisconsin. The model used was a super-learning model, leading to <em>Harris 287 - Trump 251</em>. The predictors used were polling, economic fundamentals, and lagged vote share of 2016 and 2020 in a pooled model.</p>
<p><strong>Week 5:</strong> In week 5, I used a linear regression model with the predictors lagged popular vote, latest poll average, mean poll average, a weighted average of turnout, GDP quarterly growth, and RDPI quarterly growth. The results of two separate models led to predictions that added to above 100% for the combined vote share of the two parties, which I handled by comparing the two. My goal in this case was to get at solely electoral college, but updated this with a binomial model will be explored below. This was once again a pooled model. The results led to an outcome of <em>Harris 292 - Trump 246</em> in the updated simulations.</p>
<p><strong>Week 6:</strong> This week maintained the same prediction from last week, but urged inclusion of FEC contributions data in future models to see if it improves fit. The prediction remained <em>Harris 292 - Trump 246</em>.</p>
<p><strong>Week 7:</strong> This week evaluated the results of model sin weeks 1-6, similar to the above. It also introduced the binomial simulations model, which predicted turnout as a weighted average of linear and GAM predictions. The simulations drawing from the binomial distribution using polling averages and standard deviations as hyperparameters and size of draws of the voter eligible population predicted above, this resulted in <em>Harris 270 - Trump 268</em>. This was my closest prediction of any week, with the result in any of the battleground states being within the margin of error. The result of this year’s election was therefore contained within the prediction interval of this model.</p>
<p><strong>Week 8:</strong> Making small tweaks to the binomial simulations model, I ended up with the same results as week 7, <em>Harris 270 - Trump 268</em>, with the results in all battleground states being within the margin of error. This week’s prediction therefore also captured the actual result.</p>
<p><strong>Week 9:</strong> For my 2024 election prediction, I explored various models to project presidential race outcomes, ultimately deciding on a linear regression model to predict vote margins after initial technical issues with a logistic regression and binomial simulations approach. My final model draws on predictors like polling data, voter turnout, past vote shares, and economic indicators such as GDP and income growth interacted with incumbency. Simulation results showed close races in swing states, with Democratic victories projected in Wisconsin, Michigan, Nevada, Pennsylvania, Georgia, and North Carolina, while Arizona leaned Republican. Predictive intervals indicated significant uncertainty in the swing states, but my model ultimately suggested a likely Democratic win, forecasting 308 Electoral College votes for Harris and 230 for Trump. While data limitations prevented me from incorporating ensembling, this simplified linear model provides a moderate accuracy level, with promising projections for Democratic outcomes across swing states. The actual results of the election were captured within the predictive intervals for each swing state.</p>
<p>While none of these models provided the exact correct result, or even winner, of the election, their predictive intervals are quite large and often include the end result. However, evaluating their accuracy more closely and making observations about where they went wrong, including on important variables, performance of past elections versus today, and turnout, can be instructive for understanding how our democracy currently functions and what brought about the end result.</p>
<p>Examining specific areas where these models diverged from reality, such as underestimating or overestimating support in particular demographic groups, can highlight biases in the data or assumptions that may need to be recalibrrated in future predictions. For instance, variations in regional turnout or unexpected shifts in key swing states may reveal trends not captured by conventional data inputs, signaling areas for future improvement in turnout models, particularly post-COVID and 2016 with Trump’s rise which may not be well suited by using years before in prediction. Evaluating these discrepancies can be particularly enlightening, as it allows us to identify potential blind spots in the models and understand how emerging societal changes impact voter behavior. Additionally, this examination provides a window into the assumptions embedded in these models, which may reflect outdated understandings of the electorate that are less applicable in today’s dynamic political climate.</p>
<p><strong>Accuracy of My Models</strong>
- description of the accuracy of the models
- apparent patterns in the accuracy
- graphics</p>
<p><strong>Understanding the Inaccuracies</strong>
- hypotheses for why the models were inaccurate in the estimates or locations where it was inaccurate
- reasons should not simply be statements of about the quality of the components of the model, e.g., “the polls were not good” or “economic growth was not a good predictor”
- hypotheses on why components of the model may not have been predictive or may not have been predictive in certain cases</p>
<p><strong>Testing My Hypothesis</strong>
- proposed quantitative tests that could test these hypotheses
- what data, if available, could allow you to test whether the reason proposed really did cause the inaccuracy in your model
- if there is no plausible test of the hypothesis, explain why</p>
<p><strong>If I Could Do It All Over</strong>
A description of how you might change your model if you were to do it again.</p>
</div>

</main>

  <footer>
  <link rel="stylesheet" href="//cdn.jsdelivr.net/npm/katex/dist/katex.min.css">
<script src="//cdn.jsdelivr.net/combine/npm/katex/dist/katex.min.js,npm/katex/dist/contrib/auto-render.min.js,npm/@xiee/utils/js/render-katex.js" defer></script>

<script src="//cdn.jsdelivr.net/npm/@xiee/utils/js/center-img.min.js" defer></script>

  
  <hr/>
  © <a href="https://yihui.org">Yihui Xie</a> 2017 &ndash; 2024 | <a href="https://github.com/yihui">Github</a> | <a href="https://twitter.com/xieyihui">Twitter</a>
  
  </footer>
  </body>
</html>

