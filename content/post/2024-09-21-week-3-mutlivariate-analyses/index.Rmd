---
title: 'Week 3: Polling-Based Models and Regularized Regressions'
author: ''
date: '2024-09-21'
slug: week-3-mutlivariate-analyses
categories: []
tags: []
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE)
```

```{r, results = 'hide'}
# Load libraries
library(car)
library(caret)
library(CVXR)
library(glmnet)
library(tidyverse)
library(knitr)
library(kableExtra)
library(plotly)
library(lubridate)
```

```{r}
####----------------------------------------------------------#
#### Read, merge, and process data
####----------------------------------------------------------#

# FiveThirtyEight polling average datasets national
d_pollav_natl <- read_csv("C:/Users/Jacqui Schlesinger/Documents/election-blog1/national_polls_1968-2024.csv") # if you run this code please change to location of data

# FiveThirtyEight polling average datasets state
d_pollav_state <- read_csv("C:/Users/Jacqui Schlesinger/Documents/election-blog1/state_polls_1968-2024.csv") # if you run this code please change to location of data

# pollster rating from 538
d_pollster_ratings <- read_csv("C:/Users/Jacqui Schlesinger/Documents/election-blog1/pollster-ratings-combined.csv") # if you run this code please change to location of data

# polling data from 2024
d_poll_2024 <- read_csv("C:/Users/Jacqui Schlesinger/Documents/election-blog1/president_polls_2024.csv") # if you run this code please change to location of data

# polling data from 2020
d_poll_2020 <- read_csv("C:/Users/Jacqui Schlesinger/Documents/election-blog1/president_polls_2020.csv") # if you run this code please change to location of data

# polling data from 2016
d_poll_2016 <- read_csv("C:/Users/Jacqui Schlesinger/Documents/election-blog1/president_polls_2016.csv") # if you run this code please change to location of data

# popular vote data
d_vote <- read_csv("C:/Users/Jacqui Schlesinger/Documents/election-blog1/popvote_1948-2020.csv")
d_vote$party[d_vote$party == "democrat"] <- "DEM"
d_vote$party[d_vote$party == "republican"] <- "REP"

```
This week, I implement new methodologies for more complex election forecasts. I examine model documentation of experienced forecasters, explore the impact of weekly polling data on election outcomes, apply new regression techniques to polls as predictors, and integrate economic fundamentals into a combined model as the basis of my future forecast.

### Current Forecasting Methodologies
Many forecasters' current models blend polling data with economic fundamentals. Reviewing advanced breakdowns from G Elliot Morris of 538 and Nate Silver of the Silver Bulletin, I note potential methods that I have implemented or could adopt. 

Morris’s [FiveThirtyEight forecast model](https://abcnews.go.com/538/538s-2024-presidential-election-forecast-works/story?id=110867585) for the 2024 election polling averages and state correlations through geometric decay, emphasizing state similarities in voting behavior, geography and demographics as correlates. The model combines this with economic fundamentals such as employment and consumer sentiment. This combination of fundamentals and polling, with some weighting scheme within and between the two, is implemented later. FiveThirtyEight's use of Bayesian regularized regression to manage polling biases and Markov chain Monte Carlo to simulate electoral scenarios are another future area of work in my model.

In contrast, Nate Silver's 2020 [(FiveThirtyEight)](https://fivethirtyeight.com/features/how-fivethirtyeights-2020-presidential-forecast-works-and-whats-different-because-of-covid-19/) and 2024 [(Silver Bulletin)](https://www.natesilver.net/p/model-methodology-2024) models adopt a more dynamic framework that adjusts for contemporary uncertainties, particularly changes in voter turnout patterns. Silver was not a fan of Morris's model at The Economists, and [criticized the new FiveThirtyEight model](https://www.natesilver.net/p/why-i-dont-buy-538s-new-election) predicting a Biden win post-debate despite polling favoring Trump. Silver’s approach combines polling with economic conditions, incumbency, and demographic trends like shifts in voter turnout (more engaged Democrats). The model simulates thousands of election scenarios, giving more weight to polling closer to Election Day rather than fundamentals. Recent changes maintain the model's core methodology while improving accuracy based on recent voting patterns. 

Through these examples, my goal is to achieve a model that effectively combines polling data, incumbency, and economic fundamentals, utilizing Bayesian models, weighting schemes, and simulations. 


### Recent Polling Trends and the Use of Polling in Models
Polling-- originating from Francis Galton’s 1907 [Vox populi](https://www.nature.com/articles/075450a0.pdf) on the reliability of popular judgement-- aims to gauge public sentiment but faces challenges in predicting crystallized outcomes. Thousands of polls are conducted to understand the race at a given time. But with so many complications in measuring the behavior of voters, it is extremely difficult for a poll to get the "right" answer.

These difficulties of polling are highlighted in Gelman and King's ["Why are american presidential election campaign polls so variable when votes are so predictable?"](https://www-jstor-org.ezp-prod1.hul.harvard.edu/stable/194212?sid=primo) Variability in polls, as discussed in the analysis, shows that while polls fluctuate due to unforseen events, overall outcomes remain largely predictable based on fundamental factors. While voters may appear to change their minds during the campaign, they often make final decisions based on rational, long-standing preferences. Temporary polling swings often don’t reflect lasting changes in voter intentions. 

For an example of this, look at variations in the 2020 election polling, with "game-changers" indicating significant events that may have influenced polling averages temporarily.

```{r}
####----------------------------------------------------------#
#### Visualizing poll variation over time: 2020
####----------------------------------------------------------#

# polling average 2020 plus labels, 
polling_plot_2020 <- d_pollav_natl |> 
  filter(year == 2020) |> 
  ggplot(aes(x = poll_date, y = poll_support, color = party)) +
  geom_rect(xmin = as.Date("2020-08-17"), xmax = as.Date("2020-08-20"), ymin = 47.5, ymax = 100, alpha = 0.1, color = NA, fill = "grey") + 
  annotate("text", x = as.Date("2020-08-07"), y = 51.5, label = "DNC", size = 4) +
  geom_rect(xmin = as.Date("2020-08-24"), xmax = as.Date("2020-08-27"), ymin = 0, ymax = 47.2, alpha = 0.1, color = NA, fill = "grey") +
  annotate("text", x = as.Date("2020-09-04"), y = 45, label = "RNC", size = 4) +
  geom_rect(xmin = as.Date("2020-10-02"), xmax = as.Date("2020-10-12"), ymin = 0, ymax = 42.7, alpha = 0.05, color = NA, fill = "grey") +
  
  geom_point(size = 1) + 
  geom_line() + 
  
  geom_segment(x = as.Date("2020-03-12"), xend = as.Date("2020-03-12"), y = 0, yend = 44.8, linetype = "dashed", alpha = 0.4, color = "grey") +
  annotate("text", x = as.Date("2020-03-12"), y = 42.5, label = "COVID \n Market Crash", size = 3) +
  geom_segment(x = as.Date("2020-04-08"), xend = as.Date("2020-04-08"), y = 49, yend = 100, linetype = "dashed", alpha = 0.4, color = "grey") +
  annotate("text", x = as.Date("2020-03-25"), y = 51.3, label = "Bernie Ends Run", size = 3) +
  geom_segment(x = as.Date("2020-04-16"), xend = as.Date("2020-04-16"), y = 0, yend = 44, linetype = "dashed", alpha = 0.4, color = "grey") +
  annotate("text", x = as.Date("2020-04-16"), y = 44.7, label = "22 mil \n Unemployment", size = 3) +
  geom_segment(x = as.Date("2020-05-27"), xend = as.Date("2020-05-27"), y = 0, yend = 43, linetype = "dashed", alpha = 0.4, color = "grey") +
  annotate("text", x = as.Date("2020-06-05"), y = 44, label = "100k COVID Dead, \n George Floyd", size = 3) +
  
  geom_segment(x = as.Date("2020-07-14"), xend = as.Date("2020-07-14"), y = 0, yend = 50.3, linetype = "dashed", alpha = 0.4, color = "grey") +
  annotate("text", x = as.Date("2020-06-19"), y = 47.5, label = "Moderna Announces", size = 3) +
  
  geom_segment(x = as.Date("2020-09-29"), xend = as.Date("2020-09-29"), y = 50, yend = 100, linetype = "dashed", alpha = 0.4, color = "grey") +
  annotate("text", x = as.Date("2020-9-12"), y = 49.5, label = "Pres. Debate", size = 3) +
  geom_segment(x = as.Date("2020-10-07"), xend = as.Date("2020-10-07"), y = 51.7, yend = 100, linetype = "dashed", alpha = 0.4, color = "grey") +
  annotate("text", x = as.Date("2020-10-17"), y = 50.3, label = "VP Debate", size = 3) +
  geom_segment(x = as.Date("2020-10-22"), xend = as.Date("2020-10-22"), y = 52, yend = 100, linetype = "dashed", alpha = 0.4, color = "grey") +
  annotate("text", x = as.Date("2020-10-30"), y = 51.5, label = "Pres. Debate", size = 3) +
  annotate("text", x = as.Date("2020-10-15"), y = 43.7, label = "Trump Has COVID", size = 3) +
  geom_segment(x = as.Date("2020-09-18"), xend = as.Date("2020-09-18"), y = 50, yend = 100, linetype = "dashed", alpha = 0.4, color = "grey") +
  annotate("text", x = as.Date("2020-09-03"), y = 51.5, label = "RBG Passes", size = 3) +
  
  scale_x_date(date_labels = "%b %d") + 
  scale_color_manual(values = c("dodgerblue4", "firebrick1")) +
  labs(x = "Date",
       y = "Average Poll Approval", 
       title = "Polling Averages by Date with Game Changers, 2020") + 
  theme_classic() +
  theme(plot.title = element_text(hjust = 0.5, size = 18, face = "bold"))

polling_plot_2020

```

The variation in polls compared to the final outcome as described by Gelman and King is clearly seen here. For 2024 below, there is also a large amount of uncertainty, making modeling with weekly polling averages require advanced coefficient weighting. 

```{r}
####----------------------------------------------------------#
#### Visualizing poll variation over time: 2024
####----------------------------------------------------------#

# create the 2024 polling plot
polling_plot_2024 <- d_pollav_natl |> 
  filter(year == 2024) |> 
  ggplot(aes(x = poll_date, y = poll_support, color = party)) +
  geom_point(aes(text = paste("Date:", poll_date, "<br>Support:", round(poll_support,3), "%"))) + 
  geom_line(stat= "identity") +
  scale_x_date(breaks = scales::date_breaks("1 month"), date_labels = "%b %d") +
  scale_color_manual(values = c("dodgerblue4", "firebrick1")) +
  labs(x = "Date",
       y = "Average Poll Approval", 
       title = "Polling Averages by Date, 2024") + 
  theme_classic() +
  theme(plot.title = element_text(hjust = 0.5, size = 18, face = "bold"),
      axis.text = element_text(size = 9),
      panel.grid.major = element_line(color = "lightgrey", linetype = "dashed")
  )

# convert to interactive plotly
interactive_plot_2024 <- ggplotly(polling_plot_2024, tooltip = "text")
interactive_plot_2024

```

Whether Biden's polling data can be used without adjustment as a proxy for Harris like here is an open question I will later seek to answer.

### Pollster Quality Evaluation
Different pollster methodologies and decisions impact the accuracy of polls. To account for this, FiveThirtyEight creates pollster ratings called "pollscores" which account for bias and error, and also look at transparency and percent partisan work. Variation in pollster quality can provide valuable information about the quality of the polling data my model relies on.

```{r}
####----------------------------------------------------------#
#### Pollster rating variance calcs
####----------------------------------------------------------#

# mean and 95% CI functions
mean_ci <- function(x) {
  mean_x <- mean(x, na.rm = TRUE)
  se <- sd(x, na.rm = TRUE) / sqrt(length(x))
  ci <- qt(0.95, df = length(x) - 1) * se
  c(mean = mean_x, lower = mean_x - ci, upper = mean_x + ci)
}

# get CI for variety of variables on pollsters
bias_ci <- mean_ci(d_pollster_ratings$bias_ppm)
error_ci <- mean_ci(d_pollster_ratings$error_ppm)
pollscore_ci <- mean_ci(d_pollster_ratings$POLLSCORE)
numeric_grade_ci <- mean_ci(d_pollster_ratings$numeric_grade)
wtd_avg_transparency_ci <- mean_ci(d_pollster_ratings$wtd_avg_transparency)
percent_partisan_work_ci <- mean_ci(d_pollster_ratings$percent_partisan_work)

# get min and max values for comparison column (ended up not using)
bias_min <- min(d_pollster_ratings$bias_ppm, na.rm = TRUE)
error_min <- min(d_pollster_ratings$error_ppm, na.rm = TRUE)
pollscore_min <- min(d_pollster_ratings$POLLSCORE, na.rm = TRUE)
partisan_work_min <- min(d_pollster_ratings$percent_partisan_work, na.rm = TRUE)
numeric_grade_max <- max(d_pollster_ratings$numeric_grade, na.rm = TRUE)
wtd_avg_transparency_max <- max(d_pollster_ratings$wtd_avg_transparency, na.rm = TRUE)

# put together for table
ci_data <- data.frame(
  Metric = c("Bias PPM", "Error PPM", "Pollscore", "Numeric Grade", "Weighted Avg Transparency", "Percent Partisan Work"),
  
  Mean = c(round(bias_ci["mean"], 3), round(error_ci["mean"], 3), round(pollscore_ci["mean"], 3), paste0(round(numeric_grade_ci["mean"], 3), "/3"), paste0(round(wtd_avg_transparency_ci["mean"], 3), "/10"), paste0(round(percent_partisan_work_ci["mean"] * 100, 2), "%")),
  
  Lower_CI = c(round(bias_ci["lower"], 3), round(error_ci["lower"], 3), round(pollscore_ci["lower"], 3), paste0(round(numeric_grade_ci["lower"], 3), "/3"), paste0(round(wtd_avg_transparency_ci["lower"], 3), "/10"), paste0(round(percent_partisan_work_ci["lower"] * 100, 2), "%")),
  
  Upper_CI = c(round(bias_ci["upper"], 3), round(error_ci["upper"], 3), round(pollscore_ci["upper"], 3), paste0(round(numeric_grade_ci["upper"], 3), "/3"), paste0(round(wtd_avg_transparency_ci["upper"], 3), "/10"), paste0(round(percent_partisan_work_ci["upper"] * 100, 2), "%"))
)

# table styling
kable(ci_data) %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"))
```

These summary statistics for pollster quality reveal the limits of polling data. The current number one ranked pollster (The New York Times/Siena College) has a pollscore of -1.5, the best possible, no partisan work, and 8.7/10 transparency score. It is worth noting that the Predictive Plus-Minus for pollster's absolute error, or error_ppm, is calculated as 
$$predictive\ error = adjusted\ error * (n / (n + n_{shrinkage})) + (group\ error\ prior) * (n_{shrinkage} / (n + n_{shrinkage}))$$
, and	Predictive Plus-Minus for pollster bias, or bias_ppm, is calculated as 
$$predictive\ bias = adjusted\ bias * (n / (n + n_{shrinkage})) + (group\ error\ prior) * (n_{shrinkage} / (n + n_{shrinkage}))$$
, per [538 methodology](https://abcnews.go.com/538/538s-pollster-ratings-work/story?id=105398138), where n is the time-weighted number of polls the pollster has released and n_shrinkage is an integer that represents the effective number of polls' worth of weight to put on the prior.

Through these complicated formulas, it is clear that pollster ratings are not particularly inspiring in accuracy of their poll results, at least according to these ratings. It is worth noting that transparency, which is a part of 538's model, is not necessarily a direct correlate with accuracy of results, though this is difficult to test on a poll by poll basis given the lack of true comparison values.

### A Brief Note on Regression Methodologies
Following in class evaluation of Ridge, LASSO, and elastic net regressions using cross-validation to find the optimal lambda values that minimize the MSE of predictions in week-level training data, I have decided to focus on the elastic net regularization method for this week. I will also use model ensembles to combine fundamentals and polling data such that I can weigh the different elements' impact by time until election.

### Polling-Based Regression Models
In past weeks, we have seen how simple OLS models with economic fundamentals are better than random predictors of election outcomes as univariate regressions. Now, we will use an ensemble model to weigh the results from individual polls in 2016 and 2020 by recency to 2024 election, using 2024 data as a test set after each years' elastic net regression model is created.

```{r, include = FALSE}
####----------------------------------------------------------#
#### add days and weeks left to data
####----------------------------------------------------------#

# 2016
# covert to date had to use start date because of issue with enddate and forecast date
d_poll_2016$startdate <- as.Date(d_poll_2016$startdate, format = "%m/%d/%Y")

# define election day
election_day <- as.Date("2016-11-08")

# create days left
d_poll_2016$days_left <- as.numeric(difftime(election_day, d_poll_2016$startdate, units = "days"))

# create weeks left
d_poll_2016$weeks_left <- round(d_poll_2016$days_left / 7)

# 2020
# covert to date had to use start date because of issue with enddate and forecast date
d_poll_2020$start_date <- gsub("/(\\d{2})$", "/20\\1", d_poll_2020$start_date)

d_poll_2020$start_date <- as.Date(d_poll_2020$start_date, format = "%m/%d/%Y")

# define election day
election_day <- as.Date("2020-11-03")

# create days left
d_poll_2020$days_left <- as.numeric(difftime(election_day, d_poll_2020$start_date, units = "days"))

# create weeks left
d_poll_2020$weeks_left <- round(d_poll_2020$days_left / 7)

# 2024
# covert to date had to use start date because of issue with enddate and forecast date
d_poll_2024$start_date <- gsub("/(\\d{2})$", "/20\\1", d_poll_2024$start_date)

d_poll_2024$start_date <- as.Date(d_poll_2024$start_date, format = "%m/%d/%Y")

# define election day
election_day <- as.Date("2024-11-05")

# create days left
d_poll_2024$days_left <- as.numeric(difftime(election_day, d_poll_2024$start_date, units = "days"))

# create weeks left
d_poll_2024$weeks_left <- round(d_poll_2024$days_left / 7)

```


```{r, include = FALSE}
####----------------------------------------------------------#
#### pivot data to correct format
####----------------------------------------------------------#

#2016
# pivot necessary data
d_polls_pivot_2016 <- d_poll_2016[, c("startdate", "days_left", "weeks_left", "samplesize", "rawpoll_clinton", "rawpoll_trump")]

# create polling values to match other dataset structure
d_2016_long <- d_polls_pivot_2016 %>%
  pivot_longer(cols = c(rawpoll_clinton, rawpoll_trump), 
               names_to = "candidate", 
               values_to = "poll_support")

# add party based on candidate
d_2016_long$party <- ifelse(d_2016_long$candidate == "rawpoll_clinton", "DEM", "REP")

# create final dataset
d_polls_2016_final <- d_2016_long[, c("startdate", "days_left", "weeks_left", "samplesize", "poll_support", "party")]

# add year
d_polls_2016_final$year <- 2016

# handle issues
d_polls_2016_final$weeks_left <- round(d_polls_2016_final$weeks_left)

d_polls_2016_final <- d_polls_2016_final |>
  mutate(party = ifelse(party == "Dem", "DEM", party))

d_polls_2016_final <- d_polls_2016_final |>
  mutate(party = ifelse(party == "Rep", "REP", party))

# 2020
# get wanted columns
d_polls_2020_final <- d_poll_2020[, c("start_date", "days_left", "weeks_left", "sample_size", "pct", "party")]

# rename pct to poll_support
colnames(d_polls_2020_final)[colnames(d_polls_2020_final) == "pct"] <- "poll_support"

# add year
d_polls_2020_final$year <- 2020

# handle issues
d_polls_2020_final$weeks_left <- round(d_polls_2020_final$weeks_left)

d_polls_2020_final <- d_polls_2020_final |>
  mutate(party = ifelse(party == "Dem", "DEM", party))

d_polls_2020_final <- d_polls_2020_final |>
  mutate(party = ifelse(party == "Rep", "REP", party))

d_polls_2020_final <- d_polls_2020_final %>%
  filter(party %in% c("DEM", "REP"))

# 2024
# get wanted columns
d_polls_2024_final <- d_poll_2024[, c("start_date", "days_left", "weeks_left", "sample_size", "pct", "party")]

# rename pct to poll_support
colnames(d_polls_2024_final)[colnames(d_polls_2024_final) == "pct"] <- "poll_support"

# add year
d_polls_2024_final$year <- 2024

# handle issues
d_polls_2024_final$weeks_left <- round(d_polls_2024_final$weeks_left)

d_polls_2024_final <- d_polls_2024_final |>
  mutate(party = ifelse(party == "Dem", "DEM", party))

d_polls_2024_final <- d_polls_2024_final |>
  mutate(party = ifelse(party == "Rep", "REP", party))

d_polls_2024_final <- d_polls_2024_final %>%
  filter(party %in% c("DEM", "REP"))

d_polls_2024_only <- d_polls_2024_final %>%
  filter(format(start_date, "%Y") == "2024")

```

```{r, include = FALSE, results='hide'}
####----------------------------------------------------------#
#### Holdover code from lab, mostly used for comparison to other methods as I determined which I would actually use
####----------------------------------------------------------#

# NOTE: due to the include = FALSE, results='hide' not rendering right on the blog, I have commented out summary and other lines rhat would print, but I did not end up using them in my blog analysis as I decided elastic net was the best. They use ## for clarity

# Shape and merge polling and election data using November polls. 
d_poll_nov <- d_vote |> 
  left_join(d_pollav_natl |> 
              group_by(year, party) |> 
              top_n(1, poll_date) |> 
              select(-candidate), 
            by = c("year", "party")) |> 
  rename(nov_poll = poll_support) |> 
  filter(year <= 2020) |> 
  drop_na()

# OLS: Democratic candidates pv2p on November polling average. 
ols.nov.1 <- lm(pv2p ~ nov_poll, 
                data = subset(d_poll_nov, party == "DEM"))
## summary(ols.nov.1)

# OLS: Party-stacked pv2p on November polling average.
ols.nov.2 <- lm(pv2p ~ nov_poll, 
                data = d_poll_nov)
## summary(ols.nov.2)

# Create dataset of polling average by week until the election. 
d_poll_weeks <- d_pollav_natl |> 
  group_by(year, party, weeks_left) |>
  summarize(mean_poll_week = mean(poll_support)) |> 
  filter(weeks_left <= 30) |> 
  pivot_wider(names_from = weeks_left, values_from = mean_poll_week) |> 
  left_join(d_vote, by = c("year", "party"))

# Split into training and testing data based on inclusion or exclusion of 2024. 
d_poll_weeks_train <- d_poll_weeks |> 
  filter(year <= 2020)
d_poll_weeks_test <- d_poll_weeks |> 
  filter(year == 2024)

## colnames(d_poll_weeks)[3:33] <- paste0("poll_weeks_left_", 0:30)
## colnames(d_poll_weeks_train)[3:33] <- paste0("poll_weeks_left_", 0:30)
## colnames(d_poll_weeks_test)[3:33] <- paste0("poll_weeks_left_", 0:30)

# Comparison of OLS and regularized regression methods. 
ols.pollweeks <- lm(paste0("pv2p ~ ", paste0( "poll_weeks_left_", 0:30, collapse = " + ")), 
                    data = d_poll_weeks_train)
## summary(ols.pollweeks) # N.B. Inestimable: p (31) > n (30)! 

# Separate data into X and Y for training. 
x.train <- d_poll_weeks_train |>
  ungroup() |> 
  select(all_of(starts_with("poll_weeks_left_"))) |> 
  as.matrix()
y.train <- d_poll_weeks_train$pv2p
x.train
# Ridge. 
ridge.pollsweeks <- glmnet(x = x.train, y = y.train, alpha = 0) # Set ridge using alpha = 0. 

# Visualize shrinkage. 
## plot(ridge.pollsweeks, xvar = "lambda")

# Get particular coefficients. 
## coef(ridge.pollsweeks, s = 0.1)

# Lasso.
lasso.pollsweeks <- glmnet(x = x.train, y = y.train, alpha = 1) # Set lasso using alpha = 1.

# Visualize shrinkage.
## plot(lasso.pollsweeks, xvar = "lambda")

# Get particular coefficients.
## coef(lasso.pollsweeks, s = 0.1)

# Elastic net.
enet.pollsweeks <- glmnet(x = x.train, y = y.train, alpha = 0.5) # Set elastic net using alpha = 0.5.

# Visualize shrinkage.
## plot(enet.pollsweeks, xvar = "lambda")

# Can use cross-validated versions to find the optimal values of lambda that minimize the MSE of your predictions. 
# N.B. Use set.seed() and your favorite number e.g., 12345, 02138, before each CV/any stochastic call if you want your results to be stable. 
cv.ridge.pollweeks <- cv.glmnet(x = x.train, y = y.train, alpha = 0)
cv.lasso.pollweeks <- cv.glmnet(x = x.train, y = y.train, alpha = 1)
cv.enet.pollweeks <- cv.glmnet(x = x.train, y = y.train, alpha = 0.5)

# Get minimum lambda values 
lambda.min.ridge <- cv.ridge.pollweeks$lambda.min
lambda.min.lasso <- cv.lasso.pollweeks$lambda.min
lambda.min.enet <- cv.enet.pollweeks$lambda.min

# Predict on training data using lambda values that minimize MSE.
mse.ridge <- mean((predict(ridge.pollsweeks, s = lambda.min.ridge, newx = x.train) - y.train)^2)
mse.lasso <- mean((predict(lasso.pollsweeks, s = lambda.min.lasso, newx = x.train) - y.train)^2)
mse.enet <- mean((predict(enet.pollsweeks, s = lambda.min.enet, newx = x.train) - y.train)^2)

```

```{r, include = FALSE}
####----------------------------------------------------------#
#### 2016 model comparisons-- chosen models
####----------------------------------------------------------#

# create dataset of polling average by week until the election
d_poll_weeks_2016 <- d_polls_2016_final |> 
  group_by(year, party, weeks_left) |>
  summarize(mean_poll_week = mean(poll_support)) |> 
  filter(weeks_left <= 30) |> 
  pivot_wider(names_from = weeks_left, values_from = mean_poll_week) |>
  left_join(d_vote, by = c("year", "party"))

# used chat gpt to help split data
# Split into training and testing data based on inclusion or exclusion of 2024. 
train_indices <- sample(seq_len(nrow(d_poll_weeks_2016)), size = floor(0.7 * nrow(d_poll_weeks_2016)))

d_poll_weeks_train_2016 <- d_poll_weeks_2016[train_indices, ]
d_poll_weeks_test_2016 <- d_poll_weeks_2016[-train_indices, ]

colnames(d_poll_weeks_2016)[3:33] <- paste0("poll_weeks_left_", 0:30)
colnames(d_poll_weeks_test_2016)[3:33] <- paste0("poll_weeks_left_", 0:30)
colnames(d_poll_weeks_train_2016)[3:33] <- paste0("poll_weeks_left_", 0:30)

# Separate data into X and Y for training
x.train <- d_poll_weeks_2016 |>
  ungroup() |> 
  select(all_of(starts_with("poll_weeks_left_"))) |> 
  as.matrix()
y.train <- d_poll_weeks_2016$pv2p

# Elastic net
enet.pollsweeks_2016 <- glmnet(x = x.train, y = y.train, alpha = 0.5) # Set elastic net using alpha = 0.5.
mse.enet <- mean((predict(enet.pollsweeks_2016, s = lambda.min.enet, newx = x.train) - y.train)^2)
cat("enet", mse.enet)

# Ridge
ridge.pollsweeks <- glmnet(x = x.train, y = y.train, alpha = 0) # Set ridge using alpha = 0. 
mse.ridge <- mean((predict(ridge.pollsweeks, s = lambda.min.ridge, newx = x.train) - y.train)^2)
cat("\nridge", mse.ridge)

# Lasso
lasso.pollsweeks <- glmnet(x = x.train, y = y.train, alpha = 1) # Set lasso using alpha = 1
mse.lasso <- mean((predict(lasso.pollsweeks, s = lambda.min.lasso, newx = x.train) - y.train)^2)
cat("\nlasso", mse.lasso)

# Generate plot comparing coefficients for each of the weeks. 
d.coefplot <- data.frame("OLS" = coef(ols.pollweeks)[-1], 
                         "Ridge" = coef(ridge.pollsweeks, s = lambda.min.ridge)[-1], 
                         "Lasso" = coef(lasso.pollsweeks, s = lambda.min.lasso)[-1], 
                         "Elastic Net" = coef(enet.pollsweeks_2016, s = lambda.min.enet)[-1]) |> 
  rownames_to_column("coef_name") |> 
  pivot_longer(cols = -coef_name, names_to = "method", values_to = "coef_est") |> 
  mutate(week = rep(0:30, each = 4))

d.coefplot[which(is.na(d.coefplot$coef_est)),]$coef_est <- 0 

d.coefplot |>
  ggplot(aes(x = coef_est, y = reorder(coef_name, -week), color = method)) +
  geom_segment(aes(xend = 0, yend = reorder(coef_name, -week)), alpha = 0.5, lty = "dashed") +
  geom_vline(aes(xintercept = 0), lty = "dashed") +   
  geom_point() + 
  labs(x = "Coefficient Estimate", 
       y = "Coefficient Name", 
       title = "Comparison of Coefficients Across Regularization Methods") + 
  theme_classic()

```

```{r, include = FALSE}
####----------------------------------------------------------#
#### 2020 model comparisons-- chosen models
####----------------------------------------------------------#

# create dataset of polling average by week until the election
d_poll_weeks_2020 <- d_polls_2020_final |> 
  group_by(year, party, weeks_left) |>
  summarize(mean_poll_week = mean(poll_support)) |> 
  filter(weeks_left <= 30) |> 
  pivot_wider(names_from = weeks_left, values_from = mean_poll_week) |>
  left_join(d_vote, by = c("year", "party"))

# used chat gpt to help split data
# Split into training and testing data based on inclusion or exclusion of 2024. 
train_indices <- sample(seq_len(nrow(d_poll_weeks_2020)), size = floor(0.7 * nrow(d_poll_weeks_2020)))

d_poll_weeks_train_2020 <- d_poll_weeks_2020[train_indices, ]
d_poll_weeks_test_2020 <- d_poll_weeks_2020[-train_indices, ]

colnames(d_poll_weeks_2020)[3:33] <- paste0("poll_weeks_left_", 0:30)
colnames(d_poll_weeks_test_2020)[3:33] <- paste0("poll_weeks_left_", 0:30)
colnames(d_poll_weeks_train_2020)[3:33] <- paste0("poll_weeks_left_", 0:30)

# Separate data into X and Y for training
x.train <- d_poll_weeks_2020 |>
  ungroup() |> 
  select(all_of(starts_with("poll_weeks_left_"))) |> 
  as.matrix()
y.train <- d_poll_weeks_2020$pv2p

# Elastic net
enet.pollsweeks_2020 <- glmnet(x = x.train, y = y.train, alpha = 0.5) # Set elastic net using alpha = 0.5.
mse.enet <- mean((predict(enet.pollsweeks_2020, s = lambda.min.enet, newx = x.train) - y.train)^2)
cat("enet", mse.enet)

# Ridge
ridge.pollsweeks <- glmnet(x = x.train, y = y.train, alpha = 0) # Set ridge using alpha = 0. 
mse.ridge <- mean((predict(ridge.pollsweeks, s = lambda.min.ridge, newx = x.train) - y.train)^2)
cat("\nridge", mse.ridge)

# Lasso
lasso.pollsweeks <- glmnet(x = x.train, y = y.train, alpha = 1) # Set lasso using alpha = 1
mse.lasso <- mean((predict(lasso.pollsweeks, s = lambda.min.lasso, newx = x.train) - y.train)^2)
cat("\nlasso", mse.lasso)

# Generate plot comparing coefficients for each of the weeks. 
d.coefplot <- data.frame("OLS" = coef(ols.pollweeks)[-1], 
                         "Ridge" = coef(ridge.pollsweeks, s = lambda.min.ridge)[-1], 
                         "Lasso" = coef(lasso.pollsweeks, s = lambda.min.lasso)[-1], 
                         "Elastic Net" = coef(enet.pollsweeks_2020, s = lambda.min.enet)[-1]) |> 
  rownames_to_column("coef_name") |> 
  pivot_longer(cols = -coef_name, names_to = "method", values_to = "coef_est") |> 
  mutate(week = rep(0:30, each = 4))

d.coefplot[which(is.na(d.coefplot$coef_est)),]$coef_est <- 0 

d.coefplot |>
  ggplot(aes(x = coef_est, y = reorder(coef_name, -week), color = method)) +
  geom_segment(aes(xend = 0, yend = reorder(coef_name, -week)), alpha = 0.5, lty = "dashed") +
  geom_vline(aes(xintercept = 0), lty = "dashed") +   
  geom_point() + 
  labs(x = "Coefficient Estimate", 
       y = "Coefficient Name", 
       title = "Comparison of Coefficients Across Regularization Methods") + 
  theme_classic()
```



```{r}
####----------------------------------------------------------#
#### creating the model from the polling only elastic net regressions
####----------------------------------------------------------#

# calculate the weight for each row based on days_left
d_poll_weeks_2024 <- d_polls_2024_final |> 
  group_by(year, party, weeks_left) |>
  summarize(mean_poll_week = mean(poll_support)) |> 
  filter(weeks_left <= 30) |> 
  pivot_wider(names_from = weeks_left, values_from = mean_poll_week)

# bring in correct column names
colnames(d_poll_weeks_2024)[3:25] <- paste0("poll_weeks_left_", 8:30)

election_day_2024 <- "2024-11-05"
today <- "2024-09-18"
days_left <- as.numeric(as.Date(election_day_2024) - as.Date(today))

# weight by dats to election
d_poll_weeks_2024 <- d_poll_weeks_2024 %>%
  mutate(poll_model_weight = 1 - (1 / sqrt(days_left)))

# Prepare predictors for 2024
weeks_to_use <- 8:30
x_2024 <- d_poll_weeks_2024 |> 
  ungroup() |> 
  select(all_of(paste0("poll_weeks_left_", weeks_to_use))) |> 
  as.matrix()

# chat gpt used for imputation
# Create a full predictor matrix for 2024 with NA for missing weeks
x_2024_full <- matrix(NA, nrow = nrow(x_2024), ncol = 31)

colnames(x_2024_full) <- paste0("poll_weeks_left_", 0:30)

# Set weeks 0 through 7 to the values from week 8
x_2024_full[, 0:8] <- x_2024[, 8]  # Copy values from week 8 directly to weeks 0-7

# Fill in the existing values from x_2024 for weeks 8 to 30
x_2024_full[, 9:31] <- x_2024

# Now you can use x_2024_full for predictions
pred_2016 <- predict(enet.pollsweeks_2016, s = lambda.min.enet, newx = x_2024_full)
pred_2020 <- predict(enet.pollsweeks_2020, s = lambda.min.enet, newx = x_2024_full)

# Combine predictions with weights from the dataset
final_prediction_2024 <- (d_poll_weeks_2024$poll_model_weight * pred_2020) + 
                          ((1 - d_poll_weeks_2024$poll_model_weight) * pred_2016)

# Create a data frame for the final predictions
final_predictions <- data.frame(
  Party = c("Democratic", "Republican"),
  Percentage = c("50.16656%", "50.08655%")
)

# Print the table using kable with a title
kable(final_predictions, 
      col.names = c("Party", "PV2P"), 
      booktabs = TRUE) %>%
  kable_styling(full_width = F, position = "left") %>%
  row_spec(0, bold = TRUE, color = "white", background = "#0072B2") %>%
  add_header_above(c("Final Predictions for 2024 Election" = 2)) %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"))
```

Compared to the single polls-alone elastic net regression run in lab (Harris 51.79268% - Trump 50.65879%), these results are quite similar despite the very different weighting scheme with the ensemble of elastic net regressions. For more model specifications, see the GitHub code. This forecast can be improved upon by adding fundamentals, as below.


### Combined Fundamentals and Polling Elastic Net Regression Model
A combined fundamentals and polling model may help get closer to the ground truth of how voters think. The trouble with this ensemble is determining what should matter more closer to election day: polls or fundamentals. I will compare two combined models to understand this decision. In the future, I will add in state-by-state results and incumbency.

```{r, include = FALSE}
####----------------------------------------------------------#
#### Model ensembling set up
####----------------------------------------------------------#

# Estimate models using polls alone, fundamentals alone, and combined fundamentals and polls. 
# Read economic data. 
d_econ <- read_csv("C:/Users/Jacqui Schlesinger/Documents/election-blog1/fred_econ.csv") |> 
  filter(quarter == 2)

# Combine datasets and create vote lags. 
d_combined <- d_econ |> 
  left_join(d_poll_weeks, by = "year") |> 
  filter(year %in% c(unique(d_vote$year), 2024)) |> 
  group_by(party) |> 
  mutate(pv2p_lag1 = lag(pv2p, 1), 
         pv2p_lag2 = lag(pv2p, 2)) |> 
  ungroup() |> 
  mutate(gdp_growth_x_incumbent = GDP_growth_quarterly * incumbent, 
         rdpi_growth_quarterly = RDPI_growth_quarterly * incumbent,
         cpi_x_incumbent = CPI * incumbent,
         unemployment_x_incumbent = unemployment * incumbent,
         sp500_x_incumbent = sp500_close * incumbent) # Generate interaction effects.
```

```{r, include = FALSE}
####----------------------------------------------------------#
#### fundamentals for ensemble
####----------------------------------------------------------#

# Create fundamentals-only dataset and split into training and test sets. 
d_fund <- d_combined |> 
  select("year", "pv2p", "GDP", "GDP_growth_quarterly", "RDPI", "RDPI_growth_quarterly", "CPI", "unemployment", "sp500_close",
         "incumbent", "gdp_growth_x_incumbent", "rdpi_growth_quarterly", "cpi_x_incumbent", "unemployment_x_incumbent", "sp500_x_incumbent", 
         "pv2p_lag1", "pv2p_lag2") 

x.train.fund <- d_fund |> 
  filter(year <= 2020) |>
  select(-c(year, pv2p)) |> 
  slice(-c(1:9)) |> 
  as.matrix()
y.train.fund <- d_fund |> 
  filter(year <= 2020) |> 
  select(pv2p) |> 
  slice(-c(1:9)) |> 
  as.matrix()
x.test.fund <- d_fund |> 
  filter(year == 2024) |> 
  select(-c(year, pv2p)) |> 
  as.matrix()

set.seed(02138)
enet.fund <- cv.glmnet(x = x.train.fund, y = y.train.fund, intercept = FALSE, alpha = 0.5)
lambda.min.enet.fund <- enet.fund$lambda.min

# Predict 2024 national pv2p share using elastic-net. 
(fund.pred <- predict(enet.fund, s = lambda.min.enet.fund, newx = x.test.fund))
```

```{r, include = FALSE}
####----------------------------------------------------------#
#### unused-- combined model set up
####----------------------------------------------------------#

# Sequester data for combined model.
d_combo <- d_combined |> 
  select("year", "pv2p", "GDP", "GDP_growth_quarterly", "RDPI", "RDPI_growth_quarterly", "CPI", "unemployment", "sp500_close",
         "incumbent", "gdp_growth_x_incumbent", "rdpi_growth_quarterly", "cpi_x_incumbent", "unemployment_x_incumbent", "sp500_x_incumbent", 
         "pv2p_lag1", "pv2p_lag2", all_of(paste0("poll_weeks_left_", 7:30))) 

x.train.combined <- d_combo |> 
  filter(year <= 2020) |> 
  select(-c(year, pv2p)) |> 
  slice(-c(1:9)) |> 
  as.matrix()
y.train.combined <- d_combo |>
  filter(year <= 2020) |> 
  select(pv2p) |> 
  slice(-c(1:9)) |> 
  as.matrix()
x.test.combined <- d_combo |>
  filter(year == 2024) |> 
  select(-c(year, pv2p)) |> 
  drop_na() |> 
  as.matrix()
  
# Estimate combined model (unweighted)
enet.combined <- cv.glmnet(x = x.train.combined, y = y.train.combined, intercept = FALSE, alpha = 0.5)
lambda.min.enet.combined <- enet.combined$lambda.min

```

```{r, include = FALSE}
####----------------------------------------------------------#
#### Ensemble options
####----------------------------------------------------------#
# Ensemble 2: Weight based on polls mattering closer to November. (Nate Silver)
election_day_2024 <- "2024-11-05"
today <- "2024-09-18"
days_left <- as.numeric(as.Date(election_day_2024) - as.Date(today))

(poll_model_weight <- 1- (1/sqrt(days_left)))
(fund_model_weight <- 1/sqrt(days_left))

(ensemble.2.pred <- final_prediction_2024 * poll_model_weight + fund.pred * fund_model_weight) 

# Ensemble 3. Weight based on fundamentals mattering closer to November. (Gelman & King, 1993)
(poll_model_weight <- 1/sqrt(days_left))
(fund_model_weight <- 1-(1/sqrt(days_left)))

(ensemble.3.pred <- final_prediction_2024 * poll_model_weight + fund.pred * fund_model_weight)

```

```{r}
# Sample data for ensemble predictions
final_predictions <- data.frame(
  Ensemble = c("Ensemble 2", "Ensemble 2", "Ensemble 3", "Ensemble 3"),
  Party = c("Democrat", "Republican", "Democrat", "Republican"),
  Prediction = c(51.71210, 50.22182, 51.31497, 50.00100),
  Poll_Weight = c(0.1443376, 0.1443376, 0.1443376, 0.8556624)
)

# make kable
kable(final_predictions[, c("Party", "Prediction")], 
                         col.names = c("Party", "Prediction")) %>%
  kable_styling(full_width = F) %>%
  row_spec(0, bold = TRUE, background = "#0072B2", color = "white") %>%
  row_spec(1:4, color = "black") %>%
  pack_rows("Polls More", 1, 2) %>%
  pack_rows("Fundamentals More", 3, 4) %>%
  add_header_above(c("Combined Model Predictions for 2024 Election" = 2)) %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"))

```

Like with the polling-only model, these show Harris slightly leading Trump, though by slim margins. Additionally, weighing polls or fundamentals more as November nears leads to the emphasized group having a weight nearly 85% of the ensemble model, a large influence that really only results in a 0.2 to 0.4 percentage point difference given the above. However, that could be the margin of victory. The similarities in prediction also indicate a variety of factors working together over one strongly correlated one. For now, my final prediction is the unweighted average of the three done here today

**Current Forecast: Harris 51.0645% - Trump 50.1031%**


### Data Sources
- Pollster Aggregate Ratings
- 2016, 2020, and 2024 Polling Results
- Popular Vote by Candidate, 1948-2020