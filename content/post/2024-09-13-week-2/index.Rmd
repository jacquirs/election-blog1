---
title: 'Week 2: Economic Fundamentals and Regression-Based Prediction'
author: Jacqui Schlesinger
date: '2024-09-13'
slug: week-2
categories: []
tags: []
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE)
```

```{r, results = 'hide'}
# Load libraries
library(ggplot2)
library(maps)
library(tidyverse)
library(plotly)
library(ggrepel)
library(car)
library(readxl)
library(purrr)
library(broom)
library(knitr)
library(kableExtra)
library(dplyr)
library(tidyr)

```

```{r}
####----------------------------------------------------------#
#### Read, merge, and process data
####----------------------------------------------------------#

# Load popular vote data 
d_popvote <- read_csv("C:/Users/Jacqui Schlesinger/Documents/election-blog1/popvote_1948-2020.csv") # if you run this code please change to location of data

# Load economic data from FRED: https://fred.stlouisfed.org. 
# Variables, units, & ranges: 
# GDP, billions $, 1947-2024
# GDP_growth_quarterly, %
# RDPI, $, 1959-2024
# RDPI_growth_quarterly, %
# CPI, $ index, 1947-2024
# unemployment, %, 1948-2024
# sp500_, $, 1927-2024 
d_fred <- read_csv("C:/Users/Jacqui Schlesinger/Documents/election-blog1/fred_econ.csv")

# Load economic data from the BEA: https://apps.bea.gov/iTable/?reqid=19&step=2&isuri=1&categories=survey#eyJhcHBpZCI6MTksInN0ZXBzIjpbMSwyLDMsM10sImRhdGEiOltbImNhdGVnb3JpZXMiLCJTdXJ2ZXkiXSxbIk5JUEFfVGFibGVfTGlzdCIsIjI2NCJdLFsiRmlyc3RfWWVhciIsIjE5NDciXSxbIkxhc3RfWWVhciIsIjIwMjQiXSxbIlNjYWxlIiwiMCJdLFsiU2VyaWVzIiwiUSJdXX0=.
# GDP, 1947-2024 (all)
# GNP
# RDPI
# Personal consumption expenditures
# Goods
# Durable goods
# Nondurable goods
# Services 
# Population (midperiod, thousands)
d_bea <- read_csv("C:/Users/Jacqui Schlesinger/Documents/election-blog1/bea_econ.csv") |> 
  rename(year = "Year",
         quarter = "Quarter", 
         gdp = "Gross domestic product", 
         gnp = "Gross national product", 
         dpi = "Disposable personal income", 
         consumption = "Personal consumption expenditures", 
         goods = "Goods", 
         durables = "Durable goods", 
         nondurables = "Nondurable goods", 
         services = "Services", 
         pop = "Population (midperiod, thousands)")

# Filter and merge data. 
d_inc_econ <- d_popvote |> 
  filter(incumbent_party == TRUE) |> 
  select(year, pv, pv2p, winner) |> 
  left_join(d_fred |> filter(quarter == 2)) |> 
  left_join(d_bea |> filter(quarter == "Q2") |> select(year, dpi))
  # N.B. two different sources of data to use, FRED & BEA. 
  # We are using second-quarter data since that is the latest 2024 release. 

# Remove 2020 data
d_inc_econ_2 <- d_inc_econ |>
  filter(year != 2020)

```

```{r}
####----------------------------------------------------------#
#### Create a theme
####----------------------------------------------------------#

# Create custom theme for line plots
scatterplot_theme <- theme_bw() + 
    theme(panel.border = element_blank(),
          plot.title = element_text(size = 15, hjust = 0.5), 
          axis.text = element_text(size = 12),
          axis.line = element_line(colour = "black"))

# Create custom theme for maps 
maps_theme <- theme_bw() + 
    theme(panel.border = element_blank(),     
          panel.grid.major = element_blank(),
          panel.grid.minor = element_blank(),
          axis.title = element_blank(),
          axis.text = element_blank(),
          axis.ticks = element_blank(),
          plot.title = element_text(size = 12),
          strip.text = element_text(size = 9),
          legend.position = "right",
          legend.text = element_text(size = 8),
          legend.title = element_text(size = 9))
```

This week, I assess how economic fundamentals at the national and state level predict the incumbent party's national popular vote percentage. To do so, I will evaluate various regression models using national economic indicators and compare these to models based on state-level data to understand the impact of sociotropic (group well-being focused) versus individual (self-focused) voting. 

Economic fundamentals, as tracked by the [St. Louis Federal Reserve](https://fred.stlouisfed.org/) and [Bureau of Economic Analysis](https://www.bea.gov/), offer insights into voter behavior and election forecasting. They can be used to model the national two-way popular vote percentage for the incumbent party, overcoming third-parties. Note that incumbents preside over the national economic conditions in the presidency, which can [directly impact their vote share](https://www.journals.uchicago.edu/doi/abs/10.1086/693908). This lends itself to retrospective economic voting, where past experiences inform voter decisions about incumbent candidates.


### Assumptions and Decisions
Before this analysis, I will note decisions restricting the data. I will analyze elections from 1952-2016, due to applicability issues of older data, and excluding 2020 due to pandemic-induced economic anomalies which skew predictions. The 2020 results may effect the 2024 outcome in other uncontrolled ways, creating bias in my model. This range might also be too broad due to changes in voter demographics, such as after the [Voting Rights Act of 1965](https://www.archives.gov/milestone-documents/voting-rights-act), and voting behavior over time, with retrospective economic voting potentially dropping in importance. Future iterations will weigh election years to adjust for these factors and reevaluate the 2020 results' inclusion.


### National Economic Predictors
National economic variables and their relationship to popular vote outcomes help define the economic model of voting behavior. Focusing on Q2 results in election years across economic variables-- based on research [compiled by Achen and Bartels](https://muse-jhu-edu.ezp-prod1.hul.harvard.edu/book/64646) on the retrospective model noting that recent events impact voter decisions more-- I will identify which predictors offer insights for forecasting 2024 results. Variables examined include GDP, GDP Growth Q2, RDPI, RDPI Growth Q2, CPI, unemployment rate, SP500 values, and DPI. 

Based on in-class analysis of effective independent variables, I begin with bivariate regression models using GDP growth and RDPI growth.

```{r}
####----------------------------------------------------------#
#### National Variable Scatterplot: Q2 GDP growth
####----------------------------------------------------------#

# fit a bivariate OLS to the data
reg_econ_2 <- lm(pv2p ~ GDP_growth_quarterly, 
                         data = d_inc_econ_2)

# scatterplot of GDP growth versus pv2p
scatterplot_gdp_growth <- d_inc_econ_2 |> 
  ggplot(aes(x = GDP_growth_quarterly, y = pv2p, label = year)) + 
  geom_text(aes(label = year, text = paste("GDP Growth:", round(GDP_growth_quarterly,3), "<br>Popular Vote:", round(pv2p,3))), 
            nudge_x = 0.02, nudge_y = 0.02, check_overlap = TRUE) + 
  geom_smooth(method = "lm", formula = y ~ x, color = "green", fill = "lightgreen", show.legend = FALSE) +
  geom_hline(yintercept = 50, lty = 2) + 
  geom_vline(xintercept = 0.01, lty = 2) + 
  labs(x = "Second Quarter GDP Growth (%)", 
       y = "Incumbent Party's National Popular Vote Share", 
       title = "Q2 GDP Growth vs Incumbent National Popular Vote Share") + 
  scatterplot_theme

# convert to plotly
scatterplot_interactive <- ggplotly(scatterplot_gdp_growth, tooltip = "text")

# handle odd smooth highlighting 
scatterplot_interactive <- scatterplot_interactive %>%
  style(
    hoverinfo = "text", 
    traces = which(sapply(scatterplot_interactive$x$data, function(trace) "text" %in% names(trace)))
  ) %>%
  layout(
    hovermode = "closest"
  )

scatterplot_interactive
```

```{r, include = FALSE}
####----------------------------------------------------------#
#### National Variable Numbers and Evaluation: Q2 GDP
####----------------------------------------------------------#
# summary of OLS results
cat("### Summary of OLS Results:\n")
print(summary(reg_econ_2))
cat("\n")

# correlation between popular vote and Q2 GDP
cat("### Correlation Between Popular Vote and Q2 GDP:\n")
correlation <- cor(d_inc_econ_2$GDP_growth_quarterly, d_inc_econ_2$pv2p)
print(correlation)
cat("\n")

# evaluate the in-sample fit/ r squared
cat("### In-Sample Fit (R-squared):\n")
r_squared <- summary(reg_econ_2)$r.squared
print(r_squared)
cat("\n")

# plot residuals
plot(d_inc_econ_2$year, d_inc_econ_2$pv2p, type = "l",
     main = "Residual Plot: True Y (Line), Predicted Y (dot) for Each Year",
     xlab = "Year", 
     ylab = "National Popular Vote Share")
points(d_inc_econ_2$year, predict(reg_econ_2, d_inc_econ_2), col = "purple")

# MSE, hard to interpret on its own, need to compare to other models
cat("### Mean Squared Error (MSE):\n")
mse <- mean((reg_econ_2$model$pv2p - reg_econ_2$fitted.values)^2)
print(mse)
cat("\n")

# RMSE, helpful with outliers
cat("### Root Mean Squared Error (RMSE):\n")
rmse <- sqrt(mse)
print(rmse)
cat("\n")

# Model Testing: Cross-Validation (1000 Runs)
out_samp_errors <- sapply(1:1000, function(i) {
  years_out_samp <- sample(d_inc_econ_2$year, 9) 
  mod <- lm(pv2p ~ GDP_growth_quarterly, 
            d_inc_econ_2[!(d_inc_econ_2$year %in% years_out_samp),])
  out_samp_pred <- predict(mod, d_inc_econ_2[d_inc_econ_2$year %in% years_out_samp,])
  out_samp_truth <- d_inc_econ_2$pv2p[d_inc_econ_2$year %in% years_out_samp]
  mean(out_samp_pred - out_samp_truth)
})

cat("### Mean Absolute Cross-Validation Error:\n")
mean_abs_error <- mean(abs(out_samp_errors))
print(mean_abs_error)
cat("\n")

hist(out_samp_errors, main = "Histogram of Cross-Validation Errors Q2 GDP",
     xlab = "Cross-Validation Error", ylab = "Frequency")
```

```{r, include = FALSE}
####----------------------------------------------------------#
#### 2024 Prediction: Q2 GDP National
####----------------------------------------------------------#
# Sequester 2024 data.
GDP_new <- d_fred |> 
  filter(year == 2024 & quarter == 2) |> 
  select(GDP_growth_quarterly)

# Predict uncertainty.
# makes 95% confidence interval unknown what type of distribution 
predict(reg_econ_2, GDP_new, interval = "prediction")
```

```{r}
####----------------------------------------------------------#
#### National Variable Scatterplot: Q2 RDPI growth
####----------------------------------------------------------#

# fit a bivariate OLS to the data
reg_econ_2 <- lm(pv2p ~ RDPI_growth_quarterly, 
                         data = d_inc_econ_2)

# scatterplot of RDPI growth versus pv2p
scatterplot_rdpi_growth <- d_inc_econ_2 |> 
  ggplot(aes(x = RDPI_growth_quarterly, y = pv2p, label = year)) + 
  geom_text(aes(label = year, text = paste("RDPI Growth:", round(RDPI_growth_quarterly,3), "<br>Popular Vote:", round(pv2p,3))), 
            nudge_x = 0.02, nudge_y = 0.02, check_overlap = TRUE) + 
  geom_smooth(method = "lm", formula = y ~ x, color = "green", fill = "lightgreen", show.legend = FALSE) +
  geom_hline(yintercept = 50, lty = 2) + 
  geom_vline(xintercept = 0.01, lty = 2) + 
  labs(x = "Second Quarter RDPI Growth (%)", 
       y = "Incumbent Party's National Popular Vote Share", 
       title = "Q2 RDPI Growth vs Incumbent National Popular Vote Share") + 
  scatterplot_theme

# convert to plotly
scatterplot_interactive <- ggplotly(scatterplot_rdpi_growth, tooltip = "text")

# handle odd smooth highlighting 
scatterplot_interactive <- scatterplot_interactive %>%
  style(
    hoverinfo = "text", 
    traces = which(sapply(scatterplot_interactive$x$data, function(trace) "text" %in% names(trace)))
  ) %>%
  layout(
    hovermode = "closest"
  )

scatterplot_interactive
```

```{r, include = FALSE}
####----------------------------------------------------------#
#### National Variable Numbers and Evaluation: Q2 RDPI growth
####----------------------------------------------------------#
# summary of OLS results
cat("### Summary of OLS Results:\n")
print(summary(reg_econ_2))
cat("\n")

# correlation between popular vote and Q2 RDPI
cat("### Correlation Between Popular Vote and Q2 RDPI growth:\n")
correlation <- cor(d_inc_econ_2$RDPI_growth_quarterly, d_inc_econ_2$pv2p)
print(correlation)
cat("\n")

# evaluate the in-sample fit/ r squared
cat("### In-Sample Fit (R-squared):\n")
r_squared <- summary(reg_econ_2)$r.squared
print(r_squared)
cat("\n")

# plot residuals
plot(d_inc_econ_2$year, d_inc_econ_2$pv2p, type = "l",
     main = "Residual Plot: True Y (Line), Predicted Y (dot) for Each Year",
     xlab = "Year", 
     ylab = "National Popular Vote Share")
points(d_inc_econ_2$year, predict(reg_econ_2, d_inc_econ_2), col = "purple")

# MSE, hard to interpret on its own, need to compare to other models
cat("### Mean Squared Error (MSE):\n")
mse <- mean((reg_econ_2$model$pv2p - reg_econ_2$fitted.values)^2)
print(mse)
cat("\n")

# RMSE, helpful with outliers
cat("### Root Mean Squared Error (RMSE):\n")
rmse <- sqrt(mse)
print(rmse)
cat("\n")

# Model Testing: Cross-Validation (1000 Runs)
out_samp_errors <- sapply(1:1000, function(i) {
  years_out_samp <- sample(d_inc_econ_2$year, 9) 
  mod <- lm(pv2p ~ RDPI_growth_quarterly, 
            d_inc_econ_2[!(d_inc_econ_2$year %in% years_out_samp),])
  out_samp_pred <- predict(mod, d_inc_econ_2[d_inc_econ_2$year %in% years_out_samp,])
  out_samp_truth <- d_inc_econ_2$pv2p[d_inc_econ_2$year %in% years_out_samp]
  mean(out_samp_pred - out_samp_truth)
})

cat("### Mean Absolute Cross-Validation Error:\n")
mean_abs_error <- mean(abs(out_samp_errors))
print(mean_abs_error)
cat("\n")

hist(out_samp_errors, main = "Histogram of Cross-Validation Errors Q2 RDPI",
     xlab = "Cross-Validation Error", ylab = "Frequency")
```


```{r, include = FALSE}
####----------------------------------------------------------#
#### 2024 Prediction: Q2 RDPI growth National
####----------------------------------------------------------#
# Sequester 2024 data.
RDPI_growth_new <- d_fred |> 
  filter(year == 2024 & quarter == 2) |> 
  select(RDPI_growth_quarterly)

# Predict uncertainty.
# makes 95% confidence interval unknown what type of distribution 
predict(reg_econ_2, RDPI_growth_new, interval = "prediction")
```

Neither regression shows good in-sample fit: RDPI growth explains only 11.15% of the variance in incumbent popular vote share, while GDP growth fares slightly better at 32.48%. Their overall performance indicate limited predictive power, despite being the best two predictors as shown below.

Additionally, the correlation between these predictors and the popular vote (1952-2016) is moderately strong and positive: 0.570 for GDP growth and 0.334 for RDPI growth. However, correlation does not imply causation, and there is little evidence of direct causal relationships between these single predictors and popular vote share. Additional omitted variables and multivariate relationships likely play a role that I cannot control for. Notably, the models perform worse when including the outlier 2020 data.

A summary of the national economic variables used as predictors is below:

```{r}
####----------------------------------------------------------#
#### Table of all results for comparison
####----------------------------------------------------------#

# define the function to fit the regression model and compute statistics so it can be in one table
get_variable_summary <- function(variable, data, new_data) {
  # remove rows with NA values in the variable and pv2p columns
  data_clean <- data %>%
    filter(!is.na(!!sym(variable)), !is.na(pv2p))
  
  # fit bivariate regression model
  model <- lm(as.formula(paste("pv2p ~", variable)), data = data_clean)
  model_summary <- summary(model)
  
  # compute statistics for regression
  intercept <- coef(model)["(Intercept)"]
  slope <- coef(model)[variable]
  r_squared <- model_summary$r.squared
  correlation <- cor(data_clean[[variable]], data_clean$pv2p)
  
  # predict for pv2p 2024
  prediction <- predict(model, newdata = new_data, interval = "prediction")
  
  # compute mse and rmse errors
  mse <- mean((model$model$pv2p - model$fitted.values)^2)
  rmse <- sqrt(mse)
  
  # cross-validation
  out_samp_errors <- sapply(1:1000, function(i) {
    years_out_samp <- sample(data_clean$year, min(9, nrow(data_clean) - 1), replace = FALSE)
    mod <- lm(as.formula(paste("pv2p ~", variable)), 
              data_clean[!(data_clean$year %in% years_out_samp),])
    out_samp_pred <- predict(mod, data_clean[data_clean$year %in% years_out_samp,])
    out_samp_truth <- data_clean$pv2p[data_clean$year %in% years_out_samp]
    mean(out_samp_pred - out_samp_truth)
  })
  mean_abs_error <- mean(abs(out_samp_errors))
  
  # create result tibble
  tibble(
    variable = variable,
    r_squared = r_squared,
    prediction_2024 = prediction[1, "fit"],
    prediction_2024_upper = prediction[1, "upr"],
    prediction_2024_lower = prediction[1, "lwr"],
    mean_abs_error = mean_abs_error,
    rmse = rmse,
    correlation = correlation,
    slope = slope,
    intercept = intercept
  )
}


# list of variables to analyze
variables <- c("GDP", "GDP_growth_quarterly", "RDPI", "RDPI_growth_quarterly", 
               "CPI", "unemployment", "sp500_open", "sp500_high", "sp500_low", 
               "sp500_close", "sp500_adj_close", "sp500_volume", "dpi")

# prepare 2024 data for prediction
new_data_list <- map(variables, function(var) {
  if (var == "dpi") {
    # special handling for dpi because it is in bea not fred
    d_bea %>%
      filter(year == 2024 & quarter == "Q2") %>%
      select(all_of(var))
  } else {
    d_fred %>%
      filter(year == 2024 & quarter == 2) %>%
      select(all_of(var))
  }
})

# create a table with all results
results <- map2_df(variables, new_data_list, ~get_variable_summary(.x, d_inc_econ_2, .y))

results <- results %>%
  mutate(across(where(is.numeric), ~round(.x, 3)))

kable(results) %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"))

# help from chat gpt on creating the function and map2_df

```


None of the models are particularly strong for the incumbent vote or statistically significant in predicting future values. All 13 regressions forecast Harris' popular vote percentage between 44.216% and 51.973%, but with 95% confidence intervals including 50%, indicating non-significant results. Additional factors are also needed to understand Harris' performance given the candidate-swap, a complication I cannot accurately adjust for in bivariate cases. 

Among the models, Q2 GDP growth performs best across evaluations, showing the highest in-sample fit, smallest root mean squared error, and smallest mean absolute error in out-of-sample cross-validation. This contrasts in-class results, which suggested RDPI growth as the better predictor of incumbent margin. Predictions are also sensitive to predictor choice, so while economic fundamentals may influence vote share, there is not a direct bivariate relationship. This raises questions about whether economic fundamentals impact voting directly or indirectly, and whether sociotropic or individual voting is more significant.


### State Level Predictors
While sociotropic voting suggests national economic variables influence incumbent vote share, I also explore individual economic considerations. Some voters may focus on personal economic changes rather than national trends. To investigate this, I will examine state-level regressions using unemployment rates to see if they provide better models than national data.

```{r, include = FALSE}
####----------------------------------------------------------#
#### Bring in state by state unemployment data
####----------------------------------------------------------#

# Create a named vector to map abbreviations to state names
state_abbreviations <- c(
  DATE = "date",
  ALURN = "alabama",
  ARURN = "arkansas",
  AZURN = "arizona",
  CAURN = "california",
  COURN = "colorado",
  CTURN = "connecticut",
  DCURN = "district of columbia",
  DEURN = "delaware",
  FLURN = "florida",
  GAURN = "georgia",
  IAURN = "iowa",
  IDURN = "idaho",
  ILURN = "illinois",
  INURN = "indiana",
  KSURN = "kansas",
  KYURN = "kentucky",
  LAURN = "louisiana",
  MAURN = "massachusetts",
  MDURN = "maryland",
  MEURN = "maine",
  MIURN = "michigan",
  MNURN = "minnesota",
  MOURN = "missouri",
  MSURN = "mississippi",
  MTURN = "montana",
  NCURN = "north carolina",
  NDURN = "north dakota",
  NEURN = "nebraska",
  NHURN = "new hampshire",
  NJURN = "new jersey",
  NMURN = "new mexico",
  NVURN = "nevada",
  NYURN = "new york",
  OHURN = "ohio",
  OKURN = "oklahoma",
  ORURN = "oregon",
  PAURN = "pennsylvania",
  RIURN = "rhode island",
  SCURN = "south carolina",
  SDURN = "south dakota",
  TNURN = "tennessee",
  TXURN = "texas",
  UTURN = "utah",
  VAURN = "virginia",
  VTURN = "vermont",
  WAURN = "washington",
  WIURN = "wisconsin",
  WVURN = "west virginia",
  WYURN = "wyoming"
)

unemp_state <- read_excel("C:/Users/Jacqui Schlesinger/Documents/election-blog1/Unemp.xls") # if you run this code please change to location of data to fit your set up, I am happy to provide this file as well!

# rename columns to match state names
colnames(unemp_state) <- state_abbreviations[colnames(unemp_state)]

# handle date format
unemp_state <- unemp_state %>%
  mutate(date = as.Date(date))

# handle the dates so to group by quarter and year soon
unemp_state <- unemp_state %>%
  mutate(
    year = year(date),
    quarter = quarter(date)
  )

# group and take the average
annual_quarterly_unemp <- unemp_state %>%
  group_by(year, quarter) %>%
  summarize(across(where(is.numeric), mean, na.rm = TRUE))

# get Q2 values in each election year
election_years <- c(1976, 1980, 1984, 1988, 1992, 1996, 2000, 2004, 2008, 2012, 2016, 2020, 2024)

d_unemp_q2 <- annual_quarterly_unemp %>%
  filter(quarter == 2 & year %in% election_years)

```

```{r, include = FALSE}
####----------------------------------------------------------#
#### Bring in state by state popular vote data
####----------------------------------------------------------#

d_pvstate_wide <- read_csv("C:/Users/Jacqui Schlesinger/Documents/election-blog1/clean_wide_state_2pv_1948_2020.csv") # if you run this code please change to location of data to fit your set up

d_pvstate_wide <- d_pvstate_wide %>%
  mutate(state = tolower(state))

# help from chat gpt on this one for how to transform the data correctly 

# Define the president in office for the four years leading up to each election year
incumbents <- data.frame(
  year = c(1976, 1980, 1984, 1988, 1992, 1996, 2000, 2004, 2008, 2012, 2016, 2020),
  incumbent_party = c(
    "Republican",  # Ford (1972-1976)
    "Republican",  # Ford (1976-1980)
    "Republican",  # Reagan (1980-1984)
    "Republican",  # Reagan (1984-1988)
    "Republican",  # Bush (1988-1992)
    "Democratic",  # Clinton (1992-1996)
    "Democratic",  # Clinton (1996-2000)
    "Republican",  # Bush (2000-2004)
    "Republican",  # Bush (2004-2008)
    "Democratic",  # Obama (2008-2012)
    "Democratic",  # Obama (2012-2016)
    "Republican"  # Trump (2016-2020)
  )
)

d_pvstate_wide <- d_pvstate_wide %>%
  filter(year >= 1976)

d_pvstate_wide <- d_pvstate_wide %>%
  left_join(incumbents, by = "year")

d_pvstate_wide <- d_pvstate_wide %>%
  mutate(incumbent_pv2p = case_when(
    incumbent_party == "Democratic" ~ D_pv2p,
    incumbent_party == "Republican" ~ R_pv2p,
  ))

d_state_incumbent <- d_pvstate_wide %>%
  select(year, state, incumbent_pv2p)
```

```{r, include = FALSE}
####----------------------------------------------------------#
#### Join popular vote and unemployment data
####----------------------------------------------------------#

# get unemployment data in long format
d_unemp_q2_long <- d_unemp_q2 %>%
  pivot_longer(
    cols = starts_with("alabama"):starts_with("wyoming"),
    names_to = "state",
    values_to = "unemployment_rate"
  )

# join data together
d_state_data <- d_unemp_q2_long %>%
  inner_join(d_state_incumbent, by = c("year", "state"))

```

```{r}
####----------------------------------------------------------#
#### Regressions for each state
####----------------------------------------------------------#

# function to fit the model and extract R-squared and slope, used chatgpt for tibble part and cur_data from purrr
get_regression_stats <- function(data) {
  model <- lm(incumbent_pv2p ~ unemployment_rate, data = data)
  model_summary <- summary(model)
  
  tibble(
    r_squared = model_summary$r.squared,
    slope = coef(model)["unemployment_rate"],
    intercept = coef(model)["(Intercept)"],
    slope_se = sqrt(diag(vcov(model))["unemployment_rate"])
  )
}

# Perform regression for each state and summarize results
regression_summary <- d_state_data %>%
  group_by(state) %>%
  summarize(
    # Apply function to fit model and get statistics
    stats = list(get_regression_stats(cur_data())),
    .groups = 'drop'
  ) %>%
  unnest(stats)

# start handling data for mapping
regression_summary$region <- tolower(regression_summary$state)

# sequester shapefile of states from maps library
states_map <- map_data("state")

# join data from mapping 
states_with_stats <- regression_summary |>
  left_join(states_map, by = "region")

# create base ggplot
base_map <- states_with_stats %>%
  ggplot(aes(long, lat, group = group)) +
  geom_polygon(aes(fill = r_squared,
                   text = paste("State:", state,
                                "<br>R-squared:", round(r_squared, 2),
                                "<br>Standard Error:", round(slope_se, 2),
                                "<br>Pop Vote = ", round(intercept, 2), " + unemployment_rate * ", round(slope, 2))),
               color = "black") +
  ggtitle("Regression Statistics by State") + 
  scale_fill_gradient2(high = "darkgreen", 
                       mid = "lightgreen", 
                       low = "white", 
                       name = "In Sample Fit (R-Squared)") +
  maps_theme

# use plotly to make interactive
plotly_map <- ggplotly(base_map, tooltip = "text")
plotly_map
```

Visualizing these 48 regressions (excluding Hawaii, Alaska, and Washington DC due to data limitations) shows varied in-sample fit values similar to that across national economic fundamentals. State-wide Q2 unemployment rate from 1976-2016 has varying correlation to incumbent vote share-- stronger than national GDP growth in states like North Dakota and near-zero in states like Michigan.

Without state-level GDP growth data, I cannot compare its performance in sociotropic vs individual settings. The state-by-state unemployment rate, with an average in-sample fit of 0.132, outperforms the national unemployment rate's near-zero $R^2$ but falls short to national GDP growth.

```{r, include = FALSE}
####----------------------------------------------------------#
#### 2024 prediction based on the states
####----------------------------------------------------------#

# 2024 unemployment data and pivot to long format
d_unemp_2024 <- d_unemp_q2 %>%
  filter(year == 2024) %>%
  pivot_longer(
    cols = starts_with("alabama"):starts_with("wyoming"),
    names_to = "state",
    values_to = "unemployment_rate"
  )

predictions_2024 <- d_unemp_2024 %>%
  left_join(regression_summary, by = "state") %>%
  mutate(
    # calculate the predicted popular vote
    predicted_pv2p = intercept + slope * unemployment_rate,
    
    # calculate 95% confidence intervals
    ci_lower = predicted_pv2p - 1.96 * (slope_se * unemployment_rate),
    ci_upper = predicted_pv2p + 1.96 * (slope_se * unemployment_rate)
  )

# calculate the average predicted popular vote for 2024
average_prediction <- predictions_2024 %>%
  summarize(
    average_predicted_pv2p = mean(predicted_pv2p, na.rm = TRUE),
    average_ci_lower = mean(ci_lower, na.rm = TRUE),
    average_ci_upper = mean(ci_upper, na.rm = TRUE)
  )

print(average_prediction)

```

Note that each regression has only twelve data points, likely leading to model overfitting. This issue also affects the national data, where the limited number of elections makes strong out-of-sample performance challenging.


### Individual Versus Sociotropic Voting Patterns
The results suggest that sociotropic voting, where people base their choices on national economic conditions affecting others, is a more likely explanation in the retrospective economic voting model than individual-based voting. This aligns with findings on the importance of national over state economic conditions.

As noted by [Achen and Bartels in "Democracy for Realists"](https://muse-jhu-edu.ezp-prod1.hul.harvard.edu/book/64646), voters may be retrospective but focus on short-term economic outcomes to make choices, supporting the use of Q2 election-year metrics. [Scholars Lenz and Healy](https://www.journals.uchicago.edu/doi/abs/10.1086/692785) also point out that economic evaluations during election years may favor economic manipulators over true leaders, asking whether voter decision-making is democratic. 

It is also important to consider how voter behaviors have changed. Voters may have shifted away from retrospective behavior because of party polarization. Evidence from [Dassoneville and Tien](https://www.cambridge.org/core/journals/ps-political-science-and-politics/article/abs/introduction-to-forecasting-the-2020-us-elections/78235400F6BB7E2E370214D1A2307028) highlights that such changes and economic shocks may throw off predictions.

Together, this exploration and literature analysis leads to me use national Q2 GDP growth as the predictor for my forecast.

**Current Forecast: Harris 51.585% - Trump 48.415%**

(not statistically significant)

### Data Sources
- Popular Vote by Candidate, State, 1948-2020
- FRED Economic Data, 1927-2024
- BEA Economic Data, 1947-2024