---
title: 'Week 4: Incumbency, Expert Prediction, and Super Learning in Models'
author: Jacqui Schlesinger
date: '2024-09-28'
slug: week-4
categories: []
tags: []
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE)
```

```{r}
####----------------------------------------------------------#
#### Load in libraries
####----------------------------------------------------------#

library(car)
library(caret)
library(CVXR)
library(glmnet)
library(kableExtra)
library(maps)
library(RColorBrewer)
library(sf)
library(tidyverse)
library(viridis)
library(plotly)
library(knitr)
library(kableExtra)
```

```{r}
####----------------------------------------------------------#
#### Read, merge, and process data
####----------------------------------------------------------#

# Read incumbency/vote data
d_vote <- read_csv("C:/Users/Jacqui Schlesinger/Documents/election-blog1/popvote_1948-2020.csv") # if you run this code please change to location of data

d_state_vote <- read_csv("C:/Users/Jacqui Schlesinger/Documents/election-blog1/state_popvote_1948_2020.csv") # if you run this code please change to location of data
d_vote$party[d_vote$party == "democrat"] <- "DEM"
d_vote$party[d_vote$party == "republican"] <- "REP"

# Read economic data
d_econ <- read_csv("C:/Users/Jacqui Schlesinger/Documents/election-blog1/fred_econ.csv") |> 
  filter(quarter == 2) # if you run this code please change to location of data

# Read polling and election results data
d_pollav_natl <- read_csv("C:/Users/Jacqui Schlesinger/Documents/election-blog1/national_polls_1968-2024.csv") # if you run this code please change to location of data

d_pollav_state <- read_csv("C:/Users/Jacqui Schlesinger/Documents/election-blog1/state_polls_1968-2024.csv") # if you run this code please change to location of data

# Read federal grants dataset from Kriner & Reeves (2008) 
d_pork_state <- read_csv("C:/Users/Jacqui Schlesinger/Documents/election-blog1/fedgrants_bystate_1988-2008.csv") # if you run this code please change to location of data
```

```{r}
####----------------------------------------------------------#
#### Data Manipulation
####----------------------------------------------------------#

# Shape and merge polling and election data using November polls
d_poll_nov <- d_vote |> 
  left_join(d_pollav_natl |> 
              group_by(year, party) |> 
              top_n(1, poll_date) |> 
              select(-candidate), 
            by = c("year", "party")) |> 
  rename(nov_poll = poll_support) |> 
  filter(year <= 2020) |> 
  drop_na()

# Create dataset of polling average by week until the election
d_poll_weeks <- d_pollav_natl |> 
  group_by(year, party, weeks_left) |>
  summarize(mean_poll_week = mean(poll_support)) |> 
  filter(weeks_left <= 30) |> 
  pivot_wider(names_from = weeks_left, values_from = mean_poll_week) |> 
  left_join(d_vote, by = c("year", "party"))

```

This week, I will explore the importance of several new variables to predict the two-way popular vote within contentious states in the 2024 presidential election. Key factors include incumbency measures, federal spending, and expert prediction. I finish with my new prediction using super learning.


### Historical Analysis of Incumbency
The incumbency status of a party or candidate influences how they are perceived by the electorate and popular vote outcome, as mentioned in past weeks. The core question is in the mechanism behind the incumbency advantage: is it reflective of the incumbent president alone or benefits the party in general?

```{r, include = FALSE}
####----------------------------------------------------------#
#### Descriptive statistics on the incumbency advantage
####----------------------------------------------------------#

# Approach 1: how many elections has an incumbent president won
d_vote |> 
  filter(winner) |> 
  select(year, win_party = party, win_cand = candidate) |> 
  mutate(win_party_last = lag(win_party, order_by = year),
         win_cand_last = lag(win_cand, order_by = year)) |> 
  mutate(reelect_president = win_cand_last == win_cand) |> 
  filter(year > 1948 & year < 2024) |> 
  group_by(reelect_president) |> 
  summarize(N = n()) |> 
  mutate(Percent = round(N/sum(N) * 100, 2)) |>
  as.data.frame()


# Approach 2: out of the 11 elections with incumbents winning, what happened
inc_tab <- d_vote |> 
  filter(year > 1948 & year < 2024) |>
  select(year, party, candidate, incumbent, winner) |> 
  pivot_wider(names_from = party, 
              values_from = c(candidate, incumbent, winner)) |> 
  filter(incumbent_DEM == TRUE | incumbent_REP == TRUE)


cat(paste0("Elections with At Least One Incumbent Running: ", nrow(inc_tab), "\n",
   "Incumbent Victories: ", (sum(inc_tab$incumbent_REP & inc_tab$winner_REP) + 
                             sum(inc_tab$incumbent_DEM & inc_tab$winner_DEM)), "\n",
    "Percentage: ", round((sum(inc_tab$incumbent_REP & inc_tab$winner_REP) + 
                           sum(inc_tab$incumbent_DEM & inc_tab$winner_DEM))/
                           nrow(inc_tab)*100, 2)))

# cont approach 2: in the six elections since 2000
inc_tab |> 
  filter(year >= 2000)
       
# Approach 3: how many elections has the incumbent party won 
d_vote |> 
  filter(winner) |> 
  select(year, win_party = party, win_cand = candidate) |> 
  mutate(win_party_last = lag(win_party, order_by = year),
         win_cand_last = lag(win_cand, order_by = year)) |> 
  mutate(reelect_party = win_party_last == win_party) |> 
  filter(year > 1948 & year < 2024) |> 
  group_by(reelect_party) |> 
  summarize(N = n()) |> 
  mutate(Percent = round(N/sum(N) * 100, 2)) |>
  as.data.frame()

# Approach 4: elections where the winner served in the previous admin
100*round(prop.table(table(`prev_admin` = d_vote$prev_admin[d_vote$year > 1948 & 
                                     d_vote$year < 2024 & 
                                     d_vote$winner == TRUE])), 4)


```

Analyzing presidential elections since 1952, I can aggregate the results of incumbent candidates and parties to see who benefits. Of 18 elections, only 33% have re-elected a sitting president. However, among 11 elections with incumbents, 63.64% won. In the 6 elections since 2000, three out of four incumbents were re-elected, with Donald Trump being the exception. 

While incumbent presidents show a greater than random chance of winning re-election, incumbent parties present a different narrative: they have lost 10 of the last 18 elections, and only 27.78% won when filtering for candidates from the previous administration.


### Incumbency Advantages and Disadvantages 
Diving deeper into the source of the incumbency advantage, I explore a variety of institutional factors that may impact an election in their favor. These include:

- Bully Pulpit: can shape public opinion and have media attention
- Campaigning: have a continuous campaigning advantage
- Party Nominations: challengers must fight for nomination, while incumbents conserve resources
- Powers of the Office: can control spending to win favor with constituent
- Pork Barrel Spending: can target certain districts for reelection 

While these advantages are theoretically significant, empirical evidence remains limited. Presidential spending on disasters is one that has some empirical evidence behind its impact on the presidential level.

This is explored in the literature. According to [Brown (2014)](https://www.cambridge.org/core/journals/journal-of-experimental-political-science/article/voters-dont-care-much-about-incumbency/ECFE39E003912F8AF65C2AD14A34BD8C), the electoral advantage typically linked to incumbents arises more from structural factors than from voter preferences. Brown's randomized survey experiment showed that, when controlling for these structural advantages, voters display minimal preference for incumbents over challengers. Furthermore, it demonstrated that partisanship, rather than incumbency, shapes voter preferences, with no significant evidence of incumbency fatigue or diminishing support for long-serving incumbents.

Incumbency also has theoretical disadvantages, including that polarized electorates lead partisanship to matter more as in [Donovan et al (2019)](https://fisherpub.sjf.edu/polisci_facpub/14/). Recessions and disasters are also blamed on incumbents, per [Achen and Bartel (2016)](https://www.jstor.org/stable/j.ctvc7770q). Incumbency fatigue, though disproved in Brown's study, may need to further study on its influence in this election specifically. 

In 2024, it seems acceptable to assume that the impact of incumbency for both candidates may be negligible or cancel each other out. 

### Historical Impact of Federal Spending
_Is federal spending something I should use to predict popular vote outcomes? We will find that that answer is no._  [Kriner and Reeves (2012)](https://www.cambridge.org/core/journals/journal-of-experimental-political-science/article/voters-dont-care-much-about-incumbency/ECFE39E003912F8AF65C2AD14A34BD8C) find that increased federal spending in communities boosts electoral support for incumbent presidents or their partyâ€™s nominee, with strong effects in battleground states. Federal grants serve as "electoral currency," particularly where congress-people align with the president's party. However, the electoral benefits of spending are not uniform, with liberal/moderate voters rewarding presidents more than conservatives. This underscores the nuanced role of federal spending in shaping electoral outcomes, dependent on partisan dynamics, voter ideology, and electoral context.

My analysis focuses on how federal/pork spending is distributed over time: whether presidents propose a budget that is equitable to all constituents (universalism), rewards co-partisans (partisanship), or targets electorally important constituents (electoral particularism). Then, I determine if it is worthwhile to include in my model. 

Note: core states are where the president has a large number of co-partisans.

```{r}
####----------------------------------------------------------#
#### Analysis of National Pork Spending by Type
####----------------------------------------------------------#

d_pork_state |> 
  filter(!is.na(state_year_type)) |> 
  group_by(state_year_type) |>
  summarize(mean_grant = mean(grant_mil, na.rm = T), se_grant = sd(grant_mil, na.rm = T)/sqrt(n())) |> 
  ggplot(aes(x = state_year_type, y = mean_grant, ymin = mean_grant-1.96*se_grant, ymax = mean_grant+1.96*se_grant)) + 
  coord_flip() + 
  geom_bar(stat = "identity", fill = "#38812F") + 
  geom_errorbar(aes(ymin = mean_grant - 1.96 * se_grant, ymax = mean_grant + 1.96 * se_grant), 
                width = 0.2, color = "black") + 
  geom_text(aes(label = paste0("$", round(mean_grant, 2))), 
            hjust = 0.5, vjust = -1.2, size = 3.5, fontface = "bold", color = "black") +
  geom_text(aes(y = mean_grant - 1.96 * se_grant, 
                label = paste0("$", round(mean_grant - 1.96 * se_grant, 2))), 
            vjust = 2, size = 3, color = "white") + 
  geom_text(aes(y = mean_grant + 1.96 * se_grant, 
                label = paste0("$", round(mean_grant + 1.96 * se_grant, 2))), 
            vjust = 2, size = 3, color = "black") + 
  labs(x = "Type of State & Year", 
       y = "Federal Grant Spending (Millions of $)", 
       title = "Federal Grant Spending by State Election Type") + 
  theme_classic() +
  theme(plot.title = element_text(hjust = 0.5, size = 18, face = "bold"),
        axis.text = element_text(size = 9),
        panel.grid.major = element_line(color = "lightgrey"))

```

Within each state-type, spending across years is not statistically significantly different, indicating that presidents don't simply spend more in election years, or that the budget takes more time than this. Between swing and core states, the difference is statistically significant; presidents spend more in swing states on average, 1988-2008. This disproves the co-partisan explanation, instead supporting that the budget is used to influence competitive states. Additionally, incumbents don't spend statistically significantly more on average when they versus their successors are running. This means Harris will likely not have different spending from if Biden was an incumbent candidate.

Running regressions to predict popular vote outcomes as a function of the interaction between federal grant spending and votes, including year fixed effects, does not do well on the state level. This data is also not likely relevant with its range, and therefore _will not be used in my final prediction model_.


### Expert Prediction Accuracy
A more promising variable is expert predictions from organizations like the Cook Political Report and Sabatoâ€™s Crystal Ball, which rate states on a 7-point scale for competitiveness, ranging from strong democrat (1) to strong republican (7) likelihood of victory.

```{r}
####----------------------------------------------------------#
#### Read in expert data from CPR
####----------------------------------------------------------#

# Read expert prediction data
# Read data from Cook Political Report. 
# Years: 1988-2020
# IMPORTANT: Please do not commit/push this data to GitHub or share it anywhere outside of this course!
d_cook <- read_csv("C:/Users/Jacqui Schlesinger/Documents/election-blog1/CPR_EC_Ratings.csv")[,-1] |> 
  mutate(rating_numeric = case_when(Rating == "Solid D" ~ 1,
                                    Rating == "Likely D" ~ 2,
                                    Rating == "Lean D" ~ 3,
                                    Rating == "Toss Up" ~ 4,
                                    Rating == "Lean R" ~ 5,
                                    Rating == "Likely R" ~ 6,
                                    Rating == "Solid R" ~ 7)) |> 
  mutate(solid_D = as.numeric(rating_numeric == 1),
         likely_D = as.numeric(rating_numeric == 2),
         lean_D = as.numeric(rating_numeric == 3),
         toss_up = as.numeric(rating_numeric == 4),
         lean_R = as.numeric(rating_numeric == 5),
         likely_R = as.numeric(rating_numeric == 6),
         solid_R = as.numeric(rating_numeric == 7))
# if you run this code please change to location of data

# Read data from Sabato's Crystal Ball
# Years: 2004-2024
d_sabato <- read_csv("C:/Users/Jacqui Schlesinger/Documents/election-blog1/sabato_crystal_ball_ratings.csv") |> 
  rename(state_abb = state) # if you run this code please change to location of data

state_abb_xwalk <- d_state_vote |>
  mutate(state_abb = state.abb[match(d_state_vote$state, state.name)]) |> 
  select(state, state_abb) |> 
  distinct() 
state_abb_xwalk[51,]$state <- "District of Columbia"
state_abb_xwalk[51,]$state_abb <- "DC"

d_sabato <- d_sabato |> 
  left_join(state_abb_xwalk, by = "state_abb") |>
  mutate(safe_D = as.numeric(rating == 1),
         likely_D = as.numeric(rating == 2),
         lean_D = as.numeric(rating == 3),
         toss_up = as.numeric(rating == 4),
         lean_R = as.numeric(rating == 5),
         likely_R = as.numeric(rating == 6),
         safe_R = as.numeric(rating == 7))
# Ratings: 
# 1: Safe Democrat
# 2: Likely Democrat
# 3: Lean Democrat
# 4: Toss Up
# 5: Lean Republican
# 6: Likely Republican
# 7: Safe Republican

```

```{r, include = FALSE}
####----------------------------------------------------------#
#### Compare ratings to each other
####----------------------------------------------------------#

# 2020 Comparison. 
d_sabato_2020 <- d_sabato |> 
  filter(year == 2020) |> 
  select(state, sabato_rating = rating)

d_expert_2020 <- d_cook |> 
  filter(Cycle == 2020) |> 
  select(state = State, cook_rating = rating_numeric) |> 
  left_join(d_sabato_2020, by = "state") |> 
  mutate(rating_match = as.numeric(cook_rating == sabato_rating))

d_expert_2020$state[d_expert_2020$rating_match == 0 & !is.na(d_expert_2020$sabato_rating)]
# Why the NAs? Cook makes ratings for Maine and Nebraska districts separately. 
# These may be important for the 2024 election, but are difficult to find data for. 

d_expert_2020 <- d_expert_2020 |> 
  drop_na()

d_expert_2020$rating_match |> table()

# Compare rating mismatches for 2020.
d_expert_2020[d_expert_2020$state %in% c(d_expert_2020$state[d_expert_2020$rating_match == 0]),]
```

```{r, include = FALSE}
####----------------------------------------------------------#
#### Calculate yearly match rate 
####----------------------------------------------------------#

# Combine expert predictions generally
d_expert <- d_cook |> 
  select(year = Cycle, state = State, cook_rating = rating_numeric) |> 
  left_join(d_sabato |> select(year, state, sabato_rating = rating), by = c("year", "state")) |> 
  mutate(rating_match = as.numeric(cook_rating == sabato_rating)) |> 
  drop_na()

d_expert |> 
  group_by(year) |> 
  summarize(mean_match_rate = mean(rating_match)) 

# Merge in voting data
d_state_vote_wide <- d_state_vote |> 
  select(state, year, R_pv2p, D_pv2p, R_pv2p_lag1, R_pv2p_lag2, D_pv2p_lag1, D_pv2p_lag2) |>
  mutate(margin = D_pv2p - R_pv2p, 
         winner = ifelse(D_pv2p > R_pv2p, "D", "R"))
d_state_vote_wide[d_state_vote_wide$state == "District Of Columbia",]$state <- "District of Columbia"
d_expert <- d_expert |> 
  left_join(d_state_vote_wide, 
            by = c("state", "year"))
d_expert <- d_expert |> 
  mutate(cook_correct = as.numeric((winner == "D" & cook_rating < 4) | 
                                     (winner == "R" & cook_rating > 4)),
         sabato_correct = as.numeric((winner == "D" & sabato_rating < 4) | 
                                       (winner == "R" & sabato_rating > 4)))

```

In 2020, they agreed on 42 states, including DC, and disagreed on 9. These disagreements were mostly on their level of confidence, though in the case of Georgia, Minnesota, New Hampshire, and North Carolina one rated it a toss up but the other a swing for one party. Overall, they are very similar in their predictions, and have become more similar each year since 2004 when they had about 65% match rate. 

```{r, include = FALSE}
####----------------------------------------------------------#
#### Compare general 2020 results
####----------------------------------------------------------#

# Merge in 2020 state vote data. 
d_state_vote_2020 <- d_state_vote |> 
  filter(year == 2020) |> 
  select(state, year, R_pv2p, D_pv2p) |> 
  mutate(margin = D_pv2p - R_pv2p, 
         winner = ifelse(D_pv2p > R_pv2p, "D", "R"))
d_state_vote_2020[d_state_vote_2020$state == "District Of Columbia",]$state <- "District of Columbia"

d_expert_2020 <- d_expert_2020 |>
  left_join(d_state_vote_2020, by = "state")

# See which expert group was more accurate for 2020 (counting toss ups as misses). 
d_expert_2020 <- d_expert_2020 |> 
  mutate(cook_correct = as.numeric((winner == "D" & cook_rating < 4) | 
                                   (winner == "R" & cook_rating > 4)),
         sabato_correct = as.numeric((winner == "D" & sabato_rating < 4) | 
                                     (winner == "R" & sabato_rating > 4)))

# print how many of each are correct
d_expert_2020 |>
  select(cook_correct, sabato_correct) |> 
  colMeans()

# how many 4s did each one have 
sum(d_expert_2020$sabato_rating == 4, na.rm = TRUE)
sum(d_expert_2020$cook_rating == 4, na.rm = TRUE)

# Which states did Cook miss? 
d_expert_2020[d_expert_2020$cook_correct == 0,]$state
d_expert_2020[d_expert_2020$cook_correct == 0,]

# Sabato? 
d_expert_2020[d_expert_2020$sabato_correct == 0,]$state
d_expert_2020[d_expert_2020$sabato_correct == 0,]

```

Cook's accuracy in 2020 was 88.2%, while Sabato's was 98%. Notably, Sabato had no toss-ups, which may affect their accuracy assessment. Cook has 6, all from rating states as toss-ups bearishly and therefore automatically incorrect in this case. Sabato only missed on North Carolina. If we did not count toss ups as wrong, Cooke would have 0 incorrect predictions and Sabato would have 1. In this way, they have similar accuracy in 2020.

Below I evaluate predictions using a numerical method, assessing how well experts performed in each state beyond simple correct/incorrect counts. I create a numerical method 0 to 1, 1 meaning good certain prediction (rated as strongly going to one party and it does) and 0 meaning incorrect certain prediction (rated as strongly going to one party but it goes to the other). In the future, I would add another dimension for ratings match between the two expert predictions. 

```{r}
####----------------------------------------------------------#
#### Create graph of 2020 results
####----------------------------------------------------------#

# define the 0 to 1 range and how it is calculated
correctness_score <- function(prediction, correct) {
  if (correct == 1) {
    if (prediction == 1 || prediction == 7) {
      return(1) 
    } else if (prediction == 2 || prediction == 6) {
      return(0.8) 
    } else if (prediction == 3 || prediction == 5) {
      return(0.6) 
    }
  } else if (correct == 0) {
    if (prediction == 4) {
      return(0.5)
    } else if (prediction == 3 || prediction == 5) {
      return(0.3) 
    } else if (prediction == 2 || prediction == 6) {
      return(0.1)
    } else if (prediction == 1 || prediction == 7) {
      return(0)
    }
  }
  return(NA)  # catch potential issues
}


# apply to sabato
d_expert_2020$sabato_correctness_score <- mapply(correctness_score, d_expert_2020$sabato_rating, d_expert_2020$sabato_correct)

# apply to cook
d_expert_2020$cook_correctness_score <- mapply(correctness_score, d_expert_2020$cook_rating, d_expert_2020$cook_correct)

```


```{r}
####----------------------------------------------------------#
#### Create the maps
####----------------------------------------------------------#

# create custom theme for maps 
maps_theme <- theme_bw() + 
    theme(panel.border = element_blank(),     
          panel.grid.major = element_blank(),
          panel.grid.minor = element_blank(),
          axis.title = element_blank(),
          axis.text = element_blank(),
          axis.ticks = element_blank(),
          plot.title = element_text(size = 12),
          strip.text = element_text(size = 9),
          legend.position = "right",
          legend.text = element_text(size = 8),
          legend.title = element_text(size = 9))

# sequester shapefile of states from maps library
states_map <- map_data("state")

d_expert_2020 <- d_expert_2020 %>%
  mutate(rounded_margin = round(margin,2))

# create the Cook correctness score map
cook_map <- d_expert_2020 |>
  mutate(region = tolower(state)) |>
  left_join(states_map, by = "region") |>
  ggplot(aes(long, lat, group = group)) +
  geom_polygon(aes(fill = cook_correctness_score, 
                   text = paste("State:", state, 
                                "<br>Winner:", winner, 
                                "<br>Margin:", rounded_margin, 
                                "<br>Cook Rating:", cook_rating, 
                                "<br>Cook Correct:", ifelse(cook_correct == 1, "Yes", "No"))),
               color = "black") +
  scale_fill_gradient2(
    high = "darkgreen",
    low = "white",
    limits = c(0, 1),
    name = "Correctness Rating"
  ) +
  maps_theme

# create the Sabato correctness score map
sabato_map <- d_expert_2020 |>
  mutate(region = tolower(state)) |>
  left_join(states_map, by = "region") |>
  ggplot(aes(long, lat, group = group)) +
  geom_polygon(aes(fill = sabato_correctness_score, 
                   text = paste("State:", state, 
                                "<br>Winner:", winner, 
                                "<br>Margin:", rounded_margin,
                                "<br>Sabato Rating:", sabato_rating, 
                                "<br>Sabato Correct:", ifelse(sabato_correct == 1, "Yes", "No"))),
               color = "black") +
  scale_fill_gradient2(
    high = "darkgreen",
    low = "white",
    limits = c(0, 1),
    name = "Correctness Rating"
  ) +
  maps_theme

# convert to plotly
cook_plotly <- ggplotly(cook_map, tooltip = "text")
sabato_plotly <- ggplotly(sabato_map, tooltip = "text")


# combine plots
combined_plot <- subplot(cook_plotly, sabato_plotly, nrows = 1, titleX = FALSE, titleY = FALSE)

combined_plot <- combined_plot %>%
  layout(
    title = list(text = "Comparison Between Ratings Correctness",
                 font = list(size = 16),
                 x = 0.5),
    annotations = list(
      list(x = 0.2, y = 0, text = "Cook Ratings", showarrow = FALSE, xref='paper', yref='paper',
           font = list(size = 12)),
      list(x = 0.8, y = 0, text = "Sabato Ratings", showarrow = FALSE, xref='paper', yref='paper',
           font = list(size = 12))
    ),
    height = 300,
    width = 750
  )

combined_plot

```

As shown above, both expert prediction sites did extremely well in their predictions for 2020. Their predictions appear reliable, especially for moderate-strong states. With this in mind, my state-level model will be built only in states deemed competitive in the 2024 election by expert prediction.

```{r, include = FALSE}
# 2024 Comparison:

# 2024 toss-ups? 
# 7 from Cook: https://www.cookpolitical.com/ratings/presidential-race-ratings

# 191 EV (Solid D) + 34 (Likely D) + 1 (Lean D) = 226 EV Dem
# 93 Toss Up EV
  # 11 AZ
  # 16 GA
  # 15 MI
  # 6 NV
  # 16 NC
  # 19 PA
  # 10 WI
# 148 (Solid R) + 71 (Likely R) + 0 (Lean R) = 219 EV Rep

# Sabato: 
# https://centerforpolitics.org/crystalball/2024-president/
d_sabato |> 
  filter(year == 2024 & rating == 4) |> 
  select(state)

# 226 D EV
# 93 ? EV
# 219 R EV

# Conclusion: Both have same set of states as each. 
# Importance of these swing states!

```

Both Sabato and Cook identify the same seven critical toss-up states for 2024: Arizona, Georgia, Michigan, Nevada, North Carolina, Pennsylvania, and Wisconsin. Cook has one lean-democrat, Nebraska's second district, but polling information may not be available. These are the states I will focus on going forward. This gives us a starting point of 226 Democratic and 219 Republican electoral votes. 


### This Week's Prediction Using Super Learning
Because this week focused on understanding additional variables, my final prediction is a super-learning version of last week's ensemble model of polling and economic fundamentals, only from swing states.

```{r}
####----------------------------------------------------------#
#### Super learning at the state level for swing states.
####----------------------------------------------------------#

# Split poll data into training and testing data based on inclusion or exclusion of 2024. 
d_poll_weeks_train_inc <- d_poll_weeks |> 
  filter(incumbent & year <= 2020)
d_poll_weeks_test_inc <- d_poll_weeks |> 
  filter(incumbent & year == 2024)

colnames(d_poll_weeks)[3:33] <- paste0("poll_weeks_left_", 0:30)
colnames(d_poll_weeks_train_inc)[3:33] <- paste0("poll_weeks_left_", 0:30)
colnames(d_poll_weeks_test_inc)[3:33] <- paste0("poll_weeks_left_", 0:30)

# Combine datasets and create vote lags. 
d_combined <- d_econ |> 
  left_join(d_poll_weeks, by = "year") |> 
  filter(year %in% c(unique(d_vote$year), 2024)) |> 
  group_by(party) |> 
  mutate(pv2p_lag1 = lag(pv2p, 1), 
         pv2p_lag2 = lag(pv2p, 2)) |> 
  ungroup() |> 
  mutate(gdp_growth_x_incumbent = GDP_growth_quarterly * incumbent, 
         rdpi_growth_quarterly = RDPI_growth_quarterly * incumbent,
         cpi_x_incumbent = CPI * incumbent,
         unemployment_x_incumbent = unemployment * incumbent,
         sp500_x_incumbent = sp500_close * incumbent) # Generate interaction effects.

# Sequester data for combined model.
d_combo_inc <- d_combined |> 
  filter(incumbent) |> 
  select("year", "pv2p", "GDP", "GDP_growth_quarterly", "RDPI", "RDPI_growth_quarterly", "CPI", "unemployment", "sp500_close",
         "rdpi_growth_quarterly", "pv2p_lag1", "pv2p_lag2", all_of(paste0("poll_weeks_left_", 7:30))) 

x.train.combined <- d_combo_inc |> 
  filter(year <= 2020) |> 
  select(-c(year, pv2p)) |> 
  slice(-1) |> 
  as.matrix()
y.train.combined <- d_combo_inc |>
  filter(year <= 2020) |> 
  select(pv2p) |> 
  slice(-1) |> 
  as.matrix()
x.test.combined <- d_combo_inc |>
  filter(year == 2024) |> 
  select(-c(year, pv2p)) |> 
  drop_na() |> 
  as.matrix()
  

# Get set of states where we have polling data for 2024 according to 538 poll averages.
states_2024 <- d_pollav_state$state[d_pollav_state$year == 2024] |> unique()

# Predicting for Democratic incumbents.
# Simplifications and assumptions: 
  # Assuming Harris can be treated as incumbent for 2024 (could test either)
  # Getting weights from testing models on 2020 (could do different years)
  # Pooled models (could run state-specific models)
  # Using LOO-CV on 2020 (could do K-fold CV)
  # Using average poll support across all 30 weeks until election (could do weekly support, various imputation methods)
d_state_combo <- d_pollav_state |> 
  filter((state %in% states_2024)) |> 
  group_by(year, state, party) |>
  mutate(mean_pollav = mean(poll_support)) |>
  top_n(1, poll_date) |> 
  rename(latest_pollav = poll_support) |> 
  ungroup() |> 
  left_join(d_vote |> select(-pv, -pv2p), by = c("year", "party")) |> 
  filter(party == "DEM") |> 
  left_join(d_state_vote_wide, by = c("year", "state")) 

# Model 1. Polling averages only. 
mod_1 <- lm(D_pv2p ~ latest_pollav + mean_pollav, 
            data = subset(d_state_combo, year < 2020))

# Model 2. Lagged vote model. 
mod_2 <- lm(D_pv2p ~ D_pv2p_lag1 + D_pv2p_lag2, 
            data = subset(d_state_combo, year < 2020))

# Model 3. Combined models. 
mod_3 <- lm(D_pv2p ~ incumbent + latest_pollav + mean_pollav + D_pv2p_lag1 + D_pv2p_lag2, 
            data = subset(d_state_combo, year < 2020))

# Predictions from each model. 
pred_1 <- as.numeric(predict(mod_1, newdata = subset(d_state_combo, year == 2020)))
pred_2 <- as.numeric(predict(mod_2, newdata = subset(d_state_combo, year == 2020)))
pred_3 <- as.numeric(predict(mod_3, newdata = subset(d_state_combo, year == 2020)))

# Get weights to build super learner. 
d_weight <- data.frame("truth" = d_state_combo$D_pv2p[d_state_combo$year == 2020],
                       "polls" = pred_1,
                       "lag_vote" = pred_2,
                       "combo" = pred_3)

# Constrained optimization for ensemble mod weights. 
mod_ensemble <- lm(truth ~ polls + lag_vote + combo, 
                   data = d_weight)

# Get weights and estimated weighted ensemble via constrained regression. make sum to one
c <- 3 # number of models
predictions <- cbind(pred_1, pred_2, pred_3)
y.test <- d_weight$truth
w <- lm(y.test ~ predictions-1)
beta <- Variable(c)
objective <- Minimize(sum_squares(y.test - predictions %*% beta))
prob <- Problem(objective)
constraints(prob) <- list(beta >= 0, beta <= 1)
solution_prob <- solve(prob)
weights <- solution_prob$getValue(beta)

# Predict using previous model output.
ensemble_pred <- cbind("state" = subset(d_state_combo, year == 2020)$state,
                       "pred" = round(as.numeric(t(weights) %*% t(predictions)), 3)) |> 
  as.data.frame()

ensemble_pred <- ensemble_pred |> 
  mutate(winner = ifelse(pred > 50, "D", "R"))

# states to highlight
highlight_states <- c("Arizona", "Georgia", "Michigan", "Nevada", "North Carolina", 
                      "Pennsylvania", "Wisconsin")

# create kable
kable(ensemble_pred, col.names = c("State", "D Pop Vote Prediction", "Winner")) |>
  kable_styling(
    bootstrap_options = c("striped", "hover", "condensed"),
    full_width = F
  ) |>
  row_spec(0, bold = TRUE, background = "#FFFFE0") |>
  row_spec(which(ensemble_pred$state %in% highlight_states), 
           background = "lightgreen") 

```

The results from the super-learning model for the states identified from expert prediction are in green.

**Current Forecast: Harris 303 - Trump 235**

### Data Sources
- Polls, State and National, 1968-2024
- Popular Vote and Incumbency, State and National, 1948-2020
- FRED Economic Data
- Federal Grants, State and County, 1988-2008
- Cook Political Report Ratings, 1988-2020
- Sabato's Crystal Ball Ratings, 2004-2024