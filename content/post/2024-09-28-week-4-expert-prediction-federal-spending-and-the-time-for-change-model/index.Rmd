---
title: 'Week 4: Incumbency, Expert Prediction, and Super Learning in Models'
author: Jacqui Schlesinger
date: '2024-09-28'
slug: week-4
categories: []
tags: []
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE)
```

```{r}
####----------------------------------------------------------#
#### Load in libraries
####----------------------------------------------------------#

library(car)
library(caret)
library(CVXR)
library(glmnet)
library(kableExtra)
library(maps)
library(RColorBrewer)
library(sf)
library(tidyverse)
library(viridis)
library(plotly)
library(knitr)
library(kableExtra)
```

```{r}
####----------------------------------------------------------#
#### Read, merge, and process data
####----------------------------------------------------------#

# Read incumbency/vote data
d_vote <- read_csv("C:/Users/Jacqui Schlesinger/Documents/election-blog1/popvote_1948-2020.csv") # if you run this code please change to location of data

d_state_vote <- read_csv("C:/Users/Jacqui Schlesinger/Documents/election-blog1/state_popvote_1948_2020.csv") # if you run this code please change to location of data
d_vote$party[d_vote$party == "democrat"] <- "DEM"
d_vote$party[d_vote$party == "republican"] <- "REP"

# Read economic data
d_econ <- read_csv("C:/Users/Jacqui Schlesinger/Documents/election-blog1/fred_econ.csv") |> 
  filter(quarter == 2) # if you run this code please change to location of data

# Read polling and election results data
d_pollav_natl <- read_csv("C:/Users/Jacqui Schlesinger/Documents/election-blog1/national_polls_1968-2024.csv") # if you run this code please change to location of data

d_pollav_state <- read_csv("C:/Users/Jacqui Schlesinger/Documents/election-blog1/state_polls_1968-2024.csv") # if you run this code please change to location of data

# Read federal grants dataset from Kriner & Reeves (2008) 
d_pork_state <- read_csv("C:/Users/Jacqui Schlesinger/Documents/election-blog1/fedgrants_bystate_1988-2008.csv") # if you run this code please change to location of data
```

```{r}
####----------------------------------------------------------#
#### Data Manipulation
####----------------------------------------------------------#

# Shape and merge polling and election data using November polls
d_poll_nov <- d_vote |> 
  left_join(d_pollav_natl |> 
              group_by(year, party) |> 
              top_n(1, poll_date) |> 
              select(-candidate), 
            by = c("year", "party")) |> 
  rename(nov_poll = poll_support) |> 
  filter(year <= 2020) |> 
  drop_na()

# Create dataset of polling average by week until the election
d_poll_weeks <- d_pollav_natl |> 
  group_by(year, party, weeks_left) |>
  summarize(mean_poll_week = mean(poll_support)) |> 
  filter(weeks_left <= 30) |> 
  pivot_wider(names_from = weeks_left, values_from = mean_poll_week) |> 
  left_join(d_vote, by = c("year", "party"))

```

This week, I will explore the importance of several new variables to determine how to best predict two way popular vote within contentious states in the 2024 presidential election. These include incumbency measures, federal spending, and expert prediction. I will finish with my new prediction using super learning.

### Historical Analysis of Incumbency
The incumbency status of a party or particular candidate can effect how they are perceived by the electorate and as a result their popular vote outcome, as mentioned in past weeks. The question lies in the mechanism behind the incumbent advantage or disadvantage: does it focus solely on the incumbent president and their specific actions or benefit the party of the incumbent in general? 

```{r, include = FALSE}
####----------------------------------------------------------#
#### Descriptive statistics on the incumbency advantage
####----------------------------------------------------------#

# Approach 1: how many elections has an incumbent president won
d_vote |> 
  filter(winner) |> 
  select(year, win_party = party, win_cand = candidate) |> 
  mutate(win_party_last = lag(win_party, order_by = year),
         win_cand_last = lag(win_cand, order_by = year)) |> 
  mutate(reelect_president = win_cand_last == win_cand) |> 
  filter(year > 1948 & year < 2024) |> 
  group_by(reelect_president) |> 
  summarize(N = n()) |> 
  mutate(Percent = round(N/sum(N) * 100, 2)) |>
  as.data.frame()


# Approach 2: out of the 11 elections with incumbents winning, what happened
inc_tab <- d_vote |> 
  filter(year > 1948 & year < 2024) |>
  select(year, party, candidate, incumbent, winner) |> 
  pivot_wider(names_from = party, 
              values_from = c(candidate, incumbent, winner)) |> 
  filter(incumbent_DEM == TRUE | incumbent_REP == TRUE)


cat(paste0("Elections with At Least One Incumbent Running: ", nrow(inc_tab), "\n",
   "Incumbent Victories: ", (sum(inc_tab$incumbent_REP & inc_tab$winner_REP) + 
                             sum(inc_tab$incumbent_DEM & inc_tab$winner_DEM)), "\n",
    "Percentage: ", round((sum(inc_tab$incumbent_REP & inc_tab$winner_REP) + 
                           sum(inc_tab$incumbent_DEM & inc_tab$winner_DEM))/
                           nrow(inc_tab)*100, 2)))

# cont approach 2: in the six elections since 2000
inc_tab |> 
  filter(year >= 2000)
       
# Approach 3: how many elections has the incumbent party won 
d_vote |> 
  filter(winner) |> 
  select(year, win_party = party, win_cand = candidate) |> 
  mutate(win_party_last = lag(win_party, order_by = year),
         win_cand_last = lag(win_cand, order_by = year)) |> 
  mutate(reelect_party = win_party_last == win_party) |> 
  filter(year > 1948 & year < 2024) |> 
  group_by(reelect_party) |> 
  summarize(N = n()) |> 
  mutate(Percent = round(N/sum(N) * 100, 2)) |>
  as.data.frame()

# Approach 4: elections where the winner served in the previous admin
100*round(prop.table(table(`prev_admin` = d_vote$prev_admin[d_vote$year > 1948 & 
                                     d_vote$year < 2024 & 
                                     d_vote$winner == TRUE])), 4)


```

Diving deeper into the results of presidential elections since 1952, I can aggregate the results of incumbent candidates and incumbent parties to see if they benefit from their prior positions. Of these 18 elections, only 33% have re-elected a sitting president. However, only 11 of these elections included an incumbent who was running, and 63.64% won (7 out of 11 elections). Additionally, looking only at the 6 elections since 2000, 4 of which included incumbents, 3 of the 4 won, Donald Trump being the only one who wasn't re-elected. 

While looking at incumbent presidents appears to show that incumbents have a greater than random chance of winning a re-election campaign, looking at incumbent party results does not tell the same story. The incumbent party has lost 10 of the last 18 presidential elections since 1952, and filtering for elections where the candidate served in the previous administration, only 27.78% won. 


### Incumbency Advantages and Disadvantages 
Diving deeper into the structural factors that could be a source of the incumbency advantage for candidates, I will explore a variety of institutional factors may impact the result of an election in their favor. These factors include:

- Bully Pulpit: ability to shape public opinion, as well as access to name recognition and media attention 
- Campaigning: have a time advantage, never stop campaigning 
- Party Nominations: challengers have to fight for nomination while incumbent isn't beat up and doesn’t use resources
- Powers of the Office: can control spending to gain favor with constituents
- Pork Barrel Spending: can target certain districts for reelection 

While these may be potential advantages for incumbents, these are only theoretical reasons that may help in reelection with not a lot of empirical evidence. Presidential spending on disasters in particular is one that has some empirical evidence behind its impact on the presidential level.

This is explored in the literature. According to [Brown (2014)](https://www.cambridge.org/core/journals/journal-of-experimental-political-science/article/voters-dont-care-much-about-incumbency/ECFE39E003912F8AF65C2AD14A34BD8C), the electoral advantage typically associated with incumbents is largely due to structural factors rather than voter preferences. In a randomized survey experiment, Brown found that once these structural advantages are held constant, voters exhibit minimal preference for incumbents over challengers. Furthermore, the study demonstrated that partisanship, rather than incumbency, plays a larger role in shaping voter preferences, and there is no significant evidence of incumbency fatigue or diminishing support for long-serving incumbents.

Incumbency also has theoretical disadvantages, including that polarized electorates lead partisanship to matter more than incumbency as seen in [Donovan et al (2019)](https://fisherpub.sjf.edu/polisci_facpub/14/). Recessions and disasters are also often blamed on incumbents, per [Achen and Bartel's 2016 book "Democracy for Realists"](https://www.jstor.org/stable/j.ctvc7770q). There also also some theoretical ideas of incumbency fatigue, and though Brown disproved those in his study, there may need to be further research as to how this applies to this election in particular. 

Both candidates in 2024 have name recognition and past experience with the electorate, which may be difficult to model incumbency effects exactly, but with these results it seems acceptable to assume that the impact of incumbency for both candidates may be negligible or cancel each other out. 

### Historical Impact of Federal Spending
_Is federal spending something I should use to predict popular vote outcomes? We will find that that answer is no._  [Kriner and Reeves (2012)](https://www.cambridge.org/core/journals/journal-of-experimental-political-science/article/voters-dont-care-much-about-incumbency/ECFE39E003912F8AF65C2AD14A34BD8C) find that increased federal spending in local communities boosts electoral support for incumbent presidents or their party’s nominee, with particularly strong effects in battleground states. The study highlights that federal grants serve as "electoral currency," particularly where local congressional representatives align with the president's party. However, the electoral benefits of spending are not uniform, with liberal and moderate voters rewarding presidents more than conservatives. This underscores the nuanced role of federal spending in shaping electoral outcomes, dependent on partisan dynamics, voter ideology, and electoral context.

In my own analysis, I will first look at how federal spending/ pork spending is distributed; presidents may propose a budget that is equitable, to all constituents (universalism), rewards co-partisans (partisanship), or targets electorally important constituents (electoral particularism). Then, I will determine if and how this federal spending has changed over time to see if it is worthwhile to include in my model. 

Note that below, core states are those where the president/ incumbent has a large number of co-partisans.

```{r}
####----------------------------------------------------------#
#### Analysis of National Pork Spending by Type
####----------------------------------------------------------#

d_pork_state |> 
  filter(!is.na(state_year_type)) |> 
  group_by(state_year_type) |>
  summarize(mean_grant = mean(grant_mil, na.rm = T), se_grant = sd(grant_mil, na.rm = T)/sqrt(n())) |> 
  ggplot(aes(x = state_year_type, y = mean_grant, ymin = mean_grant-1.96*se_grant, ymax = mean_grant+1.96*se_grant)) + 
  coord_flip() + 
  geom_bar(stat = "identity", fill = "#38812F") + 
  geom_errorbar(aes(ymin = mean_grant - 1.96 * se_grant, ymax = mean_grant + 1.96 * se_grant), 
                width = 0.2, color = "black") + 
  geom_text(aes(label = paste0("$", round(mean_grant, 2))), 
            hjust = 0.5, vjust = -1.2, size = 3.5, fontface = "bold", color = "black") +
  geom_text(aes(y = mean_grant - 1.96 * se_grant, 
                label = paste0("$", round(mean_grant - 1.96 * se_grant, 2))), 
            vjust = 2, size = 3, color = "white") + 
  geom_text(aes(y = mean_grant + 1.96 * se_grant, 
                label = paste0("$", round(mean_grant + 1.96 * se_grant, 2))), 
            vjust = 2, size = 3, color = "black") + 
  labs(x = "Type of State & Year", 
       y = "Federal Grant Spending (Millions of $)", 
       title = "Federal Grant Spending by State Election Type") + 
  theme_classic() +
  theme(plot.title = element_text(hjust = 0.5, size = 18, face = "bold"),
        axis.text = element_text(size = 9),
        panel.grid.major = element_line(color = "lightgrey"))

```

Within each state type, spending across types of years is not statistically significantly different, indicating that president's don't simply spend more in election years, or that the budget take much more time than this period. between swing and core states, the difference is statistically significant, meaning presidents spend more in swing states on average, 1988-2008. This disproves the co-partisans explanation for federal funding, instead supporting that the budget is used to influence more competitive states. Additionally, as shown in class, incumbents don't spend statistically significantly more on average when themselves versus their successors are running in an election year. This means Harris will likely not have different spending from what there would have been with Biden as an incumbent candidate, allowing this data to be used in predictions if it proves useful. 

However, when running regressions to predict popular vote outcomes as a function of the interaction between federal grant spending and state, including year fixed effects, the model does not do well on the state level. This data is also not likely relevant for 2024 due to it ending in 2008, and therefore _will not be used in my final prediction model_.


### Expert Prediction Accuracy
A much better variable to consider how to integrate into my model is expert prediction of the election. Organizations like the Cook Political Report and Sabato's Crystal Ball rate states by their competitiveness on a 7 point scale, ranging from strong democrat to strong republican likelihood of victory. I will first analyze the general accuracy of these two ratings organizations, then look specifically at their 2020 state level comparison. I will then draw insights that can be used for my future model. 

```{r}
####----------------------------------------------------------#
#### Read in expert data from CPR
####----------------------------------------------------------#

# Read expert prediction data
# Read data from Cook Political Report. 
# Years: 1988-2020
# IMPORTANT: Please do not commit/push this data to GitHub or share it anywhere outside of this course!
d_cook <- read_csv("C:/Users/Jacqui Schlesinger/Documents/election-blog1/CPR_EC_Ratings.csv")[,-1] |> 
  mutate(rating_numeric = case_when(Rating == "Solid D" ~ 1,
                                    Rating == "Likely D" ~ 2,
                                    Rating == "Lean D" ~ 3,
                                    Rating == "Toss Up" ~ 4,
                                    Rating == "Lean R" ~ 5,
                                    Rating == "Likely R" ~ 6,
                                    Rating == "Solid R" ~ 7)) |> 
  mutate(solid_D = as.numeric(rating_numeric == 1),
         likely_D = as.numeric(rating_numeric == 2),
         lean_D = as.numeric(rating_numeric == 3),
         toss_up = as.numeric(rating_numeric == 4),
         lean_R = as.numeric(rating_numeric == 5),
         likely_R = as.numeric(rating_numeric == 6),
         solid_R = as.numeric(rating_numeric == 7))
# if you run this code please change to location of data

# Read data from Sabato's Crystal Ball
# Years: 2004-2024
d_sabato <- read_csv("C:/Users/Jacqui Schlesinger/Documents/election-blog1/sabato_crystal_ball_ratings.csv") |> 
  rename(state_abb = state) # if you run this code please change to location of data

state_abb_xwalk <- d_state_vote |>
  mutate(state_abb = state.abb[match(d_state_vote$state, state.name)]) |> 
  select(state, state_abb) |> 
  distinct() 
state_abb_xwalk[51,]$state <- "District of Columbia"
state_abb_xwalk[51,]$state_abb <- "DC"

d_sabato <- d_sabato |> 
  left_join(state_abb_xwalk, by = "state_abb") |>
  mutate(safe_D = as.numeric(rating == 1),
         likely_D = as.numeric(rating == 2),
         lean_D = as.numeric(rating == 3),
         toss_up = as.numeric(rating == 4),
         lean_R = as.numeric(rating == 5),
         likely_R = as.numeric(rating == 6),
         safe_R = as.numeric(rating == 7))
# Ratings: 
# 1: Safe Democrat
# 2: Likely Democrat
# 3: Lean Democrat
# 4: Toss Up
# 5: Lean Republican
# 6: Likely Republican
# 7: Safe Republican

```

```{r, include = FALSE}
####----------------------------------------------------------#
#### Compare ratings to each other
####----------------------------------------------------------#

# 2020 Comparison. 
d_sabato_2020 <- d_sabato |> 
  filter(year == 2020) |> 
  select(state, sabato_rating = rating)

d_expert_2020 <- d_cook |> 
  filter(Cycle == 2020) |> 
  select(state = State, cook_rating = rating_numeric) |> 
  left_join(d_sabato_2020, by = "state") |> 
  mutate(rating_match = as.numeric(cook_rating == sabato_rating))

d_expert_2020$state[d_expert_2020$rating_match == 0 & !is.na(d_expert_2020$sabato_rating)]
# Why the NAs? Cook makes ratings for Maine and Nebraska districts separately. 
# These may be important for the 2024 election, but are difficult to find data for. 

d_expert_2020 <- d_expert_2020 |> 
  drop_na()

d_expert_2020$rating_match |> table()

# Compare rating mismatches for 2020.
d_expert_2020[d_expert_2020$state %in% c(d_expert_2020$state[d_expert_2020$rating_match == 0]),]
```

```{r, include = FALSE}
####----------------------------------------------------------#
#### Calculate yearly match rate 
####----------------------------------------------------------#

# Combine expert predictions generally
d_expert <- d_cook |> 
  select(year = Cycle, state = State, cook_rating = rating_numeric) |> 
  left_join(d_sabato |> select(year, state, sabato_rating = rating), by = c("year", "state")) |> 
  mutate(rating_match = as.numeric(cook_rating == sabato_rating)) |> 
  drop_na()

d_expert |> 
  group_by(year) |> 
  summarize(mean_match_rate = mean(rating_match)) 

# Merge in voting data
d_state_vote_wide <- d_state_vote |> 
  select(state, year, R_pv2p, D_pv2p, R_pv2p_lag1, R_pv2p_lag2, D_pv2p_lag1, D_pv2p_lag2) |>
  mutate(margin = D_pv2p - R_pv2p, 
         winner = ifelse(D_pv2p > R_pv2p, "D", "R"))
d_state_vote_wide[d_state_vote_wide$state == "District Of Columbia",]$state <- "District of Columbia"
d_expert <- d_expert |> 
  left_join(d_state_vote_wide, 
            by = c("state", "year"))
d_expert <- d_expert |> 
  mutate(cook_correct = as.numeric((winner == "D" & cook_rating < 4) | 
                                     (winner == "R" & cook_rating > 4)),
         sabato_correct = as.numeric((winner == "D" & sabato_rating < 4) | 
                                       (winner == "R" & sabato_rating > 4)))

```

To compare these two and their 2020 predictions, I will first compare them to each other. In 2020, they agreed on 42 states, including DC, and disagreed on 9. These disagreements were mostly on their level of confidence, though in the case of Georgia, Minnesota, New Hampshire, and North Carolina one rated it as a toss up but the other had it slightly swinging to one party. Overall, they were very similar in their predictions, and have becoming more similar each year since 2004 when they had about 65% match rate. 

```{r, include = FALSE}
####----------------------------------------------------------#
#### Compare general 2020 results
####----------------------------------------------------------#

# Merge in 2020 state vote data. 
d_state_vote_2020 <- d_state_vote |> 
  filter(year == 2020) |> 
  select(state, year, R_pv2p, D_pv2p) |> 
  mutate(margin = D_pv2p - R_pv2p, 
         winner = ifelse(D_pv2p > R_pv2p, "D", "R"))
d_state_vote_2020[d_state_vote_2020$state == "District Of Columbia",]$state <- "District of Columbia"

d_expert_2020 <- d_expert_2020 |>
  left_join(d_state_vote_2020, by = "state")

# See which expert group was more accurate for 2020 (counting toss ups as misses). 
d_expert_2020 <- d_expert_2020 |> 
  mutate(cook_correct = as.numeric((winner == "D" & cook_rating < 4) | 
                                   (winner == "R" & cook_rating > 4)),
         sabato_correct = as.numeric((winner == "D" & sabato_rating < 4) | 
                                     (winner == "R" & sabato_rating > 4)))

# print how many of each are correct
d_expert_2020 |>
  select(cook_correct, sabato_correct) |> 
  colMeans()

# how many 4s did each one have 
sum(d_expert_2020$sabato_rating == 4, na.rm = TRUE)
sum(d_expert_2020$cook_rating == 4, na.rm = TRUE)

# Which states did Cook miss? 
d_expert_2020[d_expert_2020$cook_correct == 0,]$state
d_expert_2020[d_expert_2020$cook_correct == 0,] |> view()

# Sabato? 
d_expert_2020[d_expert_2020$sabato_correct == 0,]$state
d_expert_2020[d_expert_2020$sabato_correct == 0,] |> view()

```

I can also compare the accuracy of each in 2020, when Cook was 88.2% correct and Sabato was 98% correct. It is worth noting that Sabato had 0 states as toss ups while Cook had 6, potentially responsible for the differences due to their bearishness in choosing outcomes. A toss up counts as an automatic incorrect in this way, which may not be the best approach. Sabato only missed on North Carolina in this case, which Cooke also got wrong, and its other five are also ones it got wrong due to rating them toss up. If we did not count toss ups as wrong, Cooke would have 0 incorrect predictions and Sabato would have 1. In this way, they seems to have similar accuracy in 2020.

Looking more closely at the predictions for 2020, I will compare how well they did in each state. For this comparison, I will need to develop a method to evaluate their performance above a simple correct/ incorrect method. To do this, I will use a numerical method 0 to 1, 1 meaning good certain prediction (rated as strongly going to one party and it does) and 0 meaning incorrect certain prediction (rated as strongly going to one party but it goes to the other). In the future, I would add another dimension for ratings match between the two expert predictions. 

```{r}
####----------------------------------------------------------#
#### Create graph of 2020 results
####----------------------------------------------------------#

# define the 0 to 1 range and how it is calculated
correctness_score <- function(prediction, correct) {
  if (correct == 1) {
    if (prediction == 1 || prediction == 7) {
      return(1) 
    } else if (prediction == 2 || prediction == 6) {
      return(0.8) 
    } else if (prediction == 3 || prediction == 5) {
      return(0.6) 
    }
  } else if (correct == 0) {
    if (prediction == 4) {
      return(0.5)
    } else if (prediction == 3 || prediction == 5) {
      return(0.3) 
    } else if (prediction == 2 || prediction == 6) {
      return(0.1)
    } else if (prediction == 1 || prediction == 7) {
      return(0)
    }
  }
  return(NA)  # catch potential issues
}


# apply to sabato
d_expert_2020$sabato_correctness_score <- mapply(correctness_score, d_expert_2020$sabato_rating, d_expert_2020$sabato_correct)

# apply to cook
d_expert_2020$cook_correctness_score <- mapply(correctness_score, d_expert_2020$cook_rating, d_expert_2020$cook_correct)

```


```{r}
####----------------------------------------------------------#
#### Create the maps
####----------------------------------------------------------#

# create custom theme for maps 
maps_theme <- theme_bw() + 
    theme(panel.border = element_blank(),     
          panel.grid.major = element_blank(),
          panel.grid.minor = element_blank(),
          axis.title = element_blank(),
          axis.text = element_blank(),
          axis.ticks = element_blank(),
          plot.title = element_text(size = 12),
          strip.text = element_text(size = 9),
          legend.position = "right",
          legend.text = element_text(size = 8),
          legend.title = element_text(size = 9))

# sequester shapefile of states from maps library
states_map <- map_data("state")

d_expert_2020 <- d_expert_2020 %>%
  mutate(rounded_margin = round(margin,2))

# create the Cook correctness score map
cook_map <- d_expert_2020 |>
  mutate(region = tolower(state)) |>
  left_join(states_map, by = "region") |>
  ggplot(aes(long, lat, group = group)) +
  geom_polygon(aes(fill = cook_correctness_score, 
                   text = paste("State:", state, 
                                "<br>Winner:", winner, 
                                "<br>Margin:", rounded_margin, 
                                "<br>Cook Rating:", cook_rating, 
                                "<br>Cook Correct:", ifelse(cook_correct == 1, "Yes", "No"))),
               color = "black") +
  scale_fill_gradient2(
    high = "darkgreen",
    low = "white",
    limits = c(0, 1),
    name = "Correctness Rating"
  ) +
  maps_theme

# create the Sabato correctness score map
sabato_map <- d_expert_2020 |>
  mutate(region = tolower(state)) |>
  left_join(states_map, by = "region") |>
  ggplot(aes(long, lat, group = group)) +
  geom_polygon(aes(fill = sabato_correctness_score, 
                   text = paste("State:", state, 
                                "<br>Winner:", winner, 
                                "<br>Margin:", rounded_margin,
                                "<br>Sabato Rating:", sabato_rating, 
                                "<br>Sabato Correct:", ifelse(sabato_correct == 1, "Yes", "No"))),
               color = "black") +
  scale_fill_gradient2(
    high = "darkgreen",
    low = "white",
    limits = c(0, 1),
    name = "Correctness Rating"
  ) +
  maps_theme

# convert to plotly
cook_plotly <- ggplotly(cook_map, tooltip = "text")
sabato_plotly <- ggplotly(sabato_map, tooltip = "text")


# combine plots
combined_plot <- subplot(cook_plotly, sabato_plotly, nrows = 1, titleX = FALSE, titleY = FALSE)

combined_plot <- combined_plot %>%
  layout(
    title = list(text = "Comparison Between Ratings Correctness",
                 font = list(size = 16),
                 x = 0.5),
    annotations = list(
      list(x = 0.2, y = 0, text = "Cook Ratings", showarrow = FALSE, xref='paper', yref='paper',
           font = list(size = 12)),
      list(x = 0.8, y = 0, text = "Sabato Ratings", showarrow = FALSE, xref='paper', yref='paper',
           font = list(size = 12))
    ),
    height = 300,
    width = 750
  )

combined_plot

```

As shown above, both expert prediction sites did extremely well in their predictions for 2020. Note that 7 on each of the ratings scales is strong democrat and 1 is strong republican. It appears that expert predictions, especially in states that are rated as moderate-strong for either party, can be counted on in 2020 or past elections to go for that party. As such, I can use this to inform my model. With this in mind, I will look at what states are considered safe versus competitive in the 2024 election, planning to build models in the future on a state by state level only for those deemed competitive. 

```{r, include = FALSE}
# 2024 Comparison:

# 2024 toss-ups? 
# 7 from Cook: https://www.cookpolitical.com/ratings/presidential-race-ratings

# 191 EV (Solid D) + 34 (Likely D) + 1 (Lean D) = 226 EV Dem
# 93 Toss Up EV
  # 11 AZ
  # 16 GA
  # 15 MI
  # 6 NV
  # 16 NC
  # 19 PA
  # 10 WI
# 148 (Solid R) + 71 (Likely R) + 0 (Lean R) = 219 EV Rep

# Sabato: 
# https://centerforpolitics.org/crystalball/2024-president/
d_sabato |> 
  filter(year == 2024 & rating == 4) |> 
  select(state)

# 226 D EV
# 93 ? EV
# 219 R EV

# Conclusion: Both have same set of states as each. 
# Importance of these swing states!

```

Sabato and Cook agree on the same seven critical toss up states for 2024: Arizona, Georgia, Michigan, Nevada, North Carolina, Pennsylvania, and Wisconsin. Cook has one learn democrat, Nebraska's second congressional district, but due to its size polling and other information might not be available. Sabato has no leans in either direction. These are the states I will be sure to focus on going forward. This gives us a starting point of 226 democratic electoral votes and 219 republican electoral votes. 

### This Week's Prediction Using Super Learning
Because this week mostly focused on understanding which additional variables to include in my future model, my final prediction will focus on a more advanced version of last week's ensemble model of polling and economic fundamentals. I will look only at swing states in this model.

```{r}
####----------------------------------------------------------#
#### Super learning at the state level for swing states.
####----------------------------------------------------------#

# Split poll data into training and testing data based on inclusion or exclusion of 2024. 
d_poll_weeks_train_inc <- d_poll_weeks |> 
  filter(incumbent & year <= 2020)
d_poll_weeks_test_inc <- d_poll_weeks |> 
  filter(incumbent & year == 2024)

colnames(d_poll_weeks)[3:33] <- paste0("poll_weeks_left_", 0:30)
colnames(d_poll_weeks_train_inc)[3:33] <- paste0("poll_weeks_left_", 0:30)
colnames(d_poll_weeks_test_inc)[3:33] <- paste0("poll_weeks_left_", 0:30)

# Combine datasets and create vote lags. 
d_combined <- d_econ |> 
  left_join(d_poll_weeks, by = "year") |> 
  filter(year %in% c(unique(d_vote$year), 2024)) |> 
  group_by(party) |> 
  mutate(pv2p_lag1 = lag(pv2p, 1), 
         pv2p_lag2 = lag(pv2p, 2)) |> 
  ungroup() |> 
  mutate(gdp_growth_x_incumbent = GDP_growth_quarterly * incumbent, 
         rdpi_growth_quarterly = RDPI_growth_quarterly * incumbent,
         cpi_x_incumbent = CPI * incumbent,
         unemployment_x_incumbent = unemployment * incumbent,
         sp500_x_incumbent = sp500_close * incumbent) # Generate interaction effects.

# Sequester data for combined model.
d_combo_inc <- d_combined |> 
  filter(incumbent) |> 
  select("year", "pv2p", "GDP", "GDP_growth_quarterly", "RDPI", "RDPI_growth_quarterly", "CPI", "unemployment", "sp500_close",
         "rdpi_growth_quarterly", "pv2p_lag1", "pv2p_lag2", all_of(paste0("poll_weeks_left_", 7:30))) 

x.train.combined <- d_combo_inc |> 
  filter(year <= 2020) |> 
  select(-c(year, pv2p)) |> 
  slice(-1) |> 
  as.matrix()
y.train.combined <- d_combo_inc |>
  filter(year <= 2020) |> 
  select(pv2p) |> 
  slice(-1) |> 
  as.matrix()
x.test.combined <- d_combo_inc |>
  filter(year == 2024) |> 
  select(-c(year, pv2p)) |> 
  drop_na() |> 
  as.matrix()
  

# Get set of states where we have polling data for 2024 according to 538 poll averages.
states_2024 <- d_pollav_state$state[d_pollav_state$year == 2024] |> unique()

# Predicting for Democratic incumbents.
# Simplifications and assumptions: 
  # Assuming Harris can be treated as incumbent for 2024 (could test either)
  # Getting weights from testing models on 2020 (could do different years)
  # Pooled models (could run state-specific models)
  # Using LOO-CV on 2020 (could do K-fold CV)
  # Using average poll support across all 30 weeks until election (could do weekly support, various imputation methods)
d_state_combo <- d_pollav_state |> 
  filter((state %in% states_2024)) |> 
  group_by(year, state, party) |>
  mutate(mean_pollav = mean(poll_support)) |>
  top_n(1, poll_date) |> 
  rename(latest_pollav = poll_support) |> 
  ungroup() |> 
  left_join(d_vote |> select(-pv, -pv2p), by = c("year", "party")) |> 
  filter(party == "DEM") |> 
  left_join(d_state_vote_wide, by = c("year", "state")) 

# Model 1. Polling averages only. 
mod_1 <- lm(D_pv2p ~ latest_pollav + mean_pollav, 
            data = subset(d_state_combo, year < 2020))

# Model 2. Lagged vote model. 
mod_2 <- lm(D_pv2p ~ D_pv2p_lag1 + D_pv2p_lag2, 
            data = subset(d_state_combo, year < 2020))

# Model 3. Combined models. 
mod_3 <- lm(D_pv2p ~ incumbent + latest_pollav + mean_pollav + D_pv2p_lag1 + D_pv2p_lag2, 
            data = subset(d_state_combo, year < 2020))

# Predictions from each model. 
pred_1 <- as.numeric(predict(mod_1, newdata = subset(d_state_combo, year == 2020)))
pred_2 <- as.numeric(predict(mod_2, newdata = subset(d_state_combo, year == 2020)))
pred_3 <- as.numeric(predict(mod_3, newdata = subset(d_state_combo, year == 2020)))

# Get weights to build super learner. 
d_weight <- data.frame("truth" = d_state_combo$D_pv2p[d_state_combo$year == 2020],
                       "polls" = pred_1,
                       "lag_vote" = pred_2,
                       "combo" = pred_3)

# Constrained optimization for ensemble mod weights. 
mod_ensemble <- lm(truth ~ polls + lag_vote + combo, 
                   data = d_weight)

# Get weights and estimated weighted ensemble via constrained regression. make sum to one
c <- 3 # number of models
predictions <- cbind(pred_1, pred_2, pred_3)
y.test <- d_weight$truth
w <- lm(y.test ~ predictions-1)
beta <- Variable(c)
objective <- Minimize(sum_squares(y.test - predictions %*% beta))
prob <- Problem(objective)
constraints(prob) <- list(beta >= 0, beta <= 1)
solution_prob <- solve(prob)
weights <- solution_prob$getValue(beta)

# Predict using previous model output.
ensemble_pred <- cbind("state" = subset(d_state_combo, year == 2020)$state,
                       "pred" = round(as.numeric(t(weights) %*% t(predictions)), 3)) |> 
  as.data.frame()

ensemble_pred <- ensemble_pred |> 
  mutate(winner = ifelse(pred > 50, "D", "R"))

# states to highlight
highlight_states <- c("Arizona", "Georgia", "Michigan", "Nevada", "North Carolina", 
                      "Pennsylvania", "Wisconsin")

# create kable
kable(ensemble_pred, col.names = c("State", "D Pop Vote Prediction", "Winner")) |>
  kable_styling(
    bootstrap_options = c("striped", "hover", "condensed"),
    full_width = F
  ) |>
  row_spec(0, bold = TRUE, background = "#FFFFE0") |>
  row_spec(which(ensemble_pred$state %in% highlight_states), 
           background = "lightgreen") 

```

The results from the super learning model for the states identified as important from expert prediction are shown in green. This leads to a prediction:

**Current Forecast: Harris 303 - Trump 235**

### Data Sources
- Polls, State and National, 1968-2024
- Popular Vote and Incumbency, State and National, 1948-2020
- FRED Economic Data
- Federal Grants, State and County, 1988-2008
- Cook Political Report Ratings, 1988-2020
- Sabato's Crystal Ball Ratings, 2004-2024